var tipuesearch = { "pages": [{"url": "/patterns/Unusual-activities", "text": "Use additional factors to notify users of unusual activities and authenticate \nwhen accounts may have been\u00a0compromised.For Internet services, prevent suspicious access to the account, and/or \nmake the account owner aware of unusual\u00a0activities.Many Internet services are using password-based authentication which is\nreally convenient (compared to strong authentication) but has apparent\u00a0drawbacks:The password itself does not\u00a0change.Unless the account owner changes the password, all sign-ins use the\nsame identity-password combination. If the password is stolen by a\nmalicious party through some means, it can be reused until it is\u00a0changed.The password can be\u00a0cracked.Many account owners tend to use weak passwords that can be\nbrute-force attacked online. Sometimes the hacker can obtain the\ndatabase dump which contains hashed passwords, which can be\nbrute-forced\u00a0offline.The account owner has no way to ensure their exclusive possession of\n    the\u00a0password.The account owner has no evidence if the password is stolen by a\nmalicious\u00a0party.The only way to control who can access is to reset the\u00a0password.As a result, if the account owner want to ensure only they have the\npassword, they can only reset the password. In some scenarios,\nresetting the password is not\u00a0enough.The password can be seen by people\u00a0around.When the account owner enters the password on a computer keyboard or\nthe screen of a mobile device (mobile phone / tablet), people around\nhave the ability to snoop the password and record\u00a0it.Thus, a service can't tell if the user is legitimate even if the\nidentity-password combination provided by the user is correct.\nFortunately, a service can usually receive some meta information to help\ndetermine if an activity is unusual. In case of such unusual activities,\na service needs to prevent suspicious access to an account, and/or\ninform the account owner of the unusual\u00a0activities.How can a service that uses username-password authentication and\ninvolves a lot of privacy identify unusual sign-ins, confirm the\nidentify of the user, and inform the account owner of such unusual\u00a0activities?The pattern described here is a tradeoff between pure password\nauthentication which is insecure, and pure multi-factor authentication\nwhich is inconvenient. It increases privacy at the cost of\u00a0usability.The fallback multi-factor authentication should be picked carefully with\nthe understanding of the\u00a0service.The strategy to identify unusual activities should also be considered\nseriously. It should be also to identify most of the activities, with a\nfalse positive rate that is not too high. It should balance the cost of\nmulti-factor\u00a0authentication.First, the service should be able to identify unusual sign-ins. Then the\nservice may use multi-factor authentication to confirm the identity of\nthe\u00a0user.The user should be informed of unusual activities, or have some ways to\nsee recent events, and even do\u00a0something.Today, a web service may appear as a website or an application on the\nuser's devices (including mobile devices and the PCs). The service can\nuse meta information to determine if a sign-in with the correct\nusername-password combination is\u00a0suspicious.The strategies described here has both false positives and false\u00a0negatives.By running native code, the application can collect some identifiers of\nthe machine, including the operating system environment settings (e.g.\nthe list of running processes), the hardware parameters (such as the ID\nof the CPU), and device UUIDs (provided by mobile operating systems like\niOS). By completing a network request, the service also retrieves the IP\naddress of the\u00a0machine.In case of a suspicious sign-in, multi-factor authentication may be a\nway to let the legitimate user in. The service can request one more\nauthentication except password, such\u00a0as:A software\u00a0tokenExamples include Google Authenticator which runs on mobile phones\nand implements RFC6238 TOTP security\u00a0tokens..A hardware token\u00a0(disconnected)Examples include a token issued by a bank which displays digits,\nwhich is similar to a software\u00a0token.A hardware token\u00a0(connected)The token may exchange a longer secondary password than the previous\none, which means it's\u00a0safer.Personal data like date of birth,\u00a0SSNObviously not a good choice here because it cannot be\u00a0changed.An one-time password (OTP) sent to the registered E-mail address /\n    mobile\u00a0phoneDepending the type of the service, maybe the user uses the same\npassword for the E-mail address, or maybe the mobile phone is stolen\nand the service runs on the mobile\u00a0phone.Using multi-factor authentication only in case of suspicious sign-ins is\nmore convenient to using it all the time, but is less\u00a0secure.When an suspicious sign-in is detected, it may be a sign that the\npassword has already been leaked. Depending on the type of the service,\nit can notify the user about the suspicious sign-in through E-mail,\ntelephone, or other\u00a0means.Here the immediate notification can also be used in the multi-factor\u00a0authentication.For services that can be logged on from multiple devices at the same\ntime, the user should be able to check the existence of other sessions,\nand review recent sign-in\u00a0events.This pattern has some limitations. For example, it relies on accurate\nidentification of suspicious sign-ins based on meta information, where\nthe meta information including the IP address can be spoofed by an\nexperienced\u00a0attacker.If the fallback multi-factor authentication only happens occasionally to\nthe legitimate account owner, they may be unprepared to such\nauthentication, leading to a decreased\u00a0usability.Gmail displays information about other sessions (if any) in the footer,\n   linking to a page named \"Activity on this account\" which lists other\n   sessions and recent activities to the Gmail account. The user has the\n   option to sign out other\u00a0sessions.In case of annoying false positives, the user may choose to disable the\n   alert for unusual activity. The disable takes about a week, \"to make\n   sure the bad guys aren't the ones who turned off your\u00a0alerts.\"It also displays all devices that are linked to the account, and allows\n   the user to unlink one or more of\u00a0them.I started with Gmail's display of account activities. It displays\nunusual activities regarding an account, which involves identifying\nunusual activities where the password entered is correct. For some other\nservices, correct passwords can be rejected from a new device /\u00a0location.So, the scope of this pattern is to handle unusual activities (including\u00a0sign-ins).This pattern includes multi-factor authentication and two-step\nauthentication, which are well studied. But the general topic about\ninforming the user of unusual activities seems to be lack of\u00a0literature.", "tags": "", "title": "Handling unusual account activities with multiple factors"},{"url": "/patterns/Added-noise-measurement-obfuscation", "text": "Add some noise to service operation measurements, but make it cancel itself in the\u00a0long-termA service provider gets continuous measurements of a service attribute linked to a service\u00a0individual.The provision of a service may require repeated, detailed measurements of a service attribute linked to a data subject to e.g. properly bill them for the service usage, or adapt the service according to the demand load. However, these measurements may reveal further information (e.g. personal habits, etc.) when repeated over\u00a0time.A noise value is added to the true, measured value before it is transmitted to the service provider, so as to obfuscate it. The noise abides by a previously known distribution, so that the best estimation for the result of adding several measurements can be computed, while an adversary would not be able to infer the real value of any individual measurement. Note that the noise needs not be either additive or Gaussian. In fact, these may not be useful for privacy-oriented obfuscation. Scaling noise and additive Laplacian noise have proved more useful for privacy\u00a0preservation.A service provider can get reliable measurements of service attributes to fulfil its operating requirements; however, no additional personal information can be inferred from the aggregation of several measurements coming from the same\u00a0user.The pattern applies to any scenario where the use of a resource over time is being monitored (e.g. smart grid, cloud computing). The device providing the measurement must be trustworthy, in order to ensure that it abides by the established noise\u00a0pattern.Some information is lost due to the noise added. This loss of information may prevent the information from being exploited for other purposes. This is partly an intended consequence (e.g. avoid discovering user habits), but it may also preclude other legitimate\u00a0uses.In order for information to be useful after noise addition, the number of data points over which measurements are aggregated (i.e. the size of the aggregated user base) needs to be high; otherwise, either the confidence interval would be too broad or differential privacy could not be effectively\u00a0achieved.The pattern applies to any scenario where the use of a resource over time is being monitored (e.g. smart grid, cloud computing). The device providing the measurement must be trustworthy, in order to ensure that it abides by the established noise\u00a0pattern.Some information is lost due to the noise added. This loss of information may prevent the information from being exploited for other purposes. This is partly an intended consequence (e.g. avoid discovering user habits), but it may also preclude other legitimate\u00a0uses.In order for information to be useful after noise addition, the number of data points over which measurements are aggregated (i.e. the size of the aggregated user base) needs to be high; otherwise, either the confidence interval would be too broad or differential privacy could not be effectively\u00a0achieved.An electric utility operates a smart grid network with smart meters that provide measurements of the instantaneous power consumption of each user. The utility employs that information to both adapt the power distribution in a dynamic fashion, according to user demand at each moment, and bill the each client periodically, according to his aggregated consumption over the billing period. However, this information can also be exploited to infer sensitive user information (e.g. at what time he or she leaves and comes back to home,\u00a0etc.)", "tags": "", "title": "Added-noise measurement obfuscation"},{"url": "/patterns/Lawful-Consent", "text": "This pattern covers in detail the legal and social obligations surrounding a data subject's consent to processing of their data in specific circumstances. Every use of the subject's personal data should be covered by an explicit agreement in which the data subject was made aware of the implications of their\u00a0consent.Where data controllers (e.g. organisations) aim to provide a service (or product) to users, there may be opportunities to reuse data, gather feedback, or make use of user data to further their system's value. Many controllers seek to continually collect and utilise this data, often in ways which warrant privacy concerns. For any data processing (including collection), controllers should first obtain consent from the users in\u00a0question.There are social norms surrounding the use of personal data which need to be adhered to if an controller wishes to avoid scrutiny. Users do not inherently trust controllers to handle their personal data with care for privacy. Without clearly defined boundaries, these users may have justifiable concerns about what is learned about them, and how this information may be used. Additionally, various jurisdictions supply varying compliance requirements, and these controllers need to cater to every market they provide\u00a0to.Doing otherwise, possibly by disinterest or negligence, may have financial consequences in addition to potential public outcry. Despite this, controllers regularly consider the impact that their decisions may have on competitive edge and resulting profits. The link between better decision making, possibly less sharing, and reduced monetary gains sways some controllers into unlawful forms of\u00a0consent.An controller aims to maximise the value of their services by gathering as much sharing and participation as possible, potentially seeing user consent as a barrier to functionality and efficiency. They may inadvertently subvert notions of consent by unnecessarily bundling together desirable services with needs for personal information, or downplaying the significance of the data involved. They undermine self-determination at the risk of losing trust from their users, and attracting legal investigations which may rule their practices\u00a0unlawful.A user should be given every opportunity to assess their sharing choices prior to making their consent. The controller should aid the user in comprehending the tradeoffs apparent in using each of their services, without over-burdening the user. These consented services should be purposed-separated, so that users may make use of functionality without first granting unnecessary\u00a0consent.Services should be separated into distinct processes for which distinct consent is acquired. Each purpose requires its own consent. These permissions need to be given subsequent to ascertaining sufficient awareness in the user about the consequences of that\u00a0consent.The users should not be pressured into providing consent. Instead, the benefits may be presented along with the trade-offs so that the users may make an informed decision. Some users are not necessarily capable of making these decisions themselves (e.g. children) and thus provisions need to be made to cater to this. The provided information should not be misleading, as coerced consent is not a valid form of permission. One way to present policies in an accessible manner is through comparative examples (e.g. in addition to further detail, what is unique about our privacy\u00a0policy?).In more personal services (i.e. one-on-one), personal privacy policies may undergo a formal negotiation. As opposed to user preferences (both at sign-up and through appropriate defaults), understanding a user's personal privacy requirements may benefit from the facilitation of a human representative. This, however, suffers from it's own drawbacks where the representative may misunderstand the user's requirements. Even in interpersonal exchanges, controllers should err on the side of caution. Where available, explicit signing of an agreement aids in proving\u00a0consent.With the ability to choose exactly what tradeoffs are agreeable to them, users will be more content, and trusting of the system. They may as a result use more services, and participate more than they otherwise would. Being aware of what information is actually needed to perform certain functionality may also prevent its use, but rightfully so as to prevent\u00a0backlash.The need for certain information for some services will bring inappropriate business processes to the foreground to be rectified, or otherwise questioned. This will likely bring the controller towards better practices, and may affect others as well. Once the public sees the controller's willingness to cooperate, trust will grow even\u00a0further.Overall adoption will grow for controllers who are shown to be trustworthy and upfront about their data processing practices. This may very well offset the costs involved in maintaining\u00a0transparency.Allowing informed and specific consent prevents controllers from soliciting misplaced consent, which greatly reduces the adoption of invasive services. These are often the most profitable\u00a0services.Explicit Consent / Obtaining Explicit consent via privacy agreement / Permission to Use Sensitive Data via Privacy Agreement / Sign an Agreement to Solve Lack of Trust on the Use of Private Data\u00a0ContextArticle 29 Data Protection Working Party. (2007). Opinion 4/2007 on the concept of personal data. Working Party Opinions. Retrieved from\u00a0http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Opinion+4+/+2007+on+the+concept+of+personal+data#0J. Porekar, A. Jerman-Bla\u017ei\u010d, and T. Klobu\u010dar, \u201cTowards organizational privacy patterns,\u201d Proceedings - The 2nd International Conference on the Digital Society, ICDS 2008,\u00a02008.C. Bier and E. Krempel, \u201cCommon Privacy Patterns in Video Surveillance and Smart Energy,\u201d in ICCCT-2012, 2012, pp.\u00a0610\u2013615.", "tags": "", "title": "Lawful Consent"},{"url": "/patterns/Private-link", "text": "The controller provides a service which hosts resources, potentially constituting personal data. When users want to share (and enable re-sharing of) these resources, they may wish to do so privately using existing communication mechanisms. This is particularly relevant when users are sharing with contacts who would rather not, or cannot, simply\u00a0authenticate.Users want to share a private resource with unauthenticated users in a way that respects the sensitivity of that resource.\nThe solution must not allow users to access resources that weren't intended to be shared, nor publicize the location of the intended resource to unintended\u00a0recipients.Note that the URL will be retained in recipients' browser history and could easily be inadvertently shared with others. Services should help users understand these\u00a0limitations.Services may allow users to revoke existing private links or change the URL to effectively re-set who can access the resource. Additionally, users may set a time-limit for the resource's validity, or have it invalidated upon\u00a0modification.The controller allows their users' online resources to be shared by publishing an unlisted URL with a complex, long, and randomly generated string. This can be part of a query string as opposed to an on disk location. In this case, the preprocessor intercepts the query and redirects the user to the correct resource. This may be an actual file on disk (probably not served by direct link), generated on the fly, or extracted from a database or compressed file. The preprocessor can verify validity dynamically before serving the\u00a0resource.The situation in which the user has a direct link to the resource location is not ideal, however, as it will need to change in the event of a time or version restriction since access to the file is not controlled by the\u00a0preprocessor.Based\u00a0on:", "tags": "", "title": "Private link"},{"url": "/patterns/Privacy-aware-network-client", "text": "A privacy policy which is hard to understand is in an automated way\nconverted into a more easy to read\u00a0format.This pattern is limited to the web browsing\u00a0domain.Many websites have privacy policies which are hard to understand for\nthe general\u00a0audience.Many people enter websites with different intentions like shopping,\nresearch, etc. At the same time those responsible collect, use and\nrelease information about a user by explaining it through statements\ncalled privacy policies. These privacy policies are not easy to read\nand to\u00a0understand.The problem can be solved by designing and implementing a privacy\nproxy that can parse and interpret policies. Afterwards it translates\nthe policies into a human-readable format to present them in a\nuser-friendly\u00a0way.Expand the awareness of the user towards privacy policies of a\u00a0website.The user's awareness of the privacy policy rises so that more informed\ndecisions can be made. The proxy is able to automatically detect\nchanges of the privacy policy. A separate secure connection is needed\nfor the proxy for every access to an area which is secured by a\nprivacy policy. Policy constraints need to allocate local storage\nin the client. An attack on this could lead the user to decisions\nwhich he would otherwise not do. If there are any breaches of\nprivacy it can be blamed on the client if he did use a\nprivacy-aware client for a particular\u00a0access.Alice uses several web services but is not aware of the their privacy\npolicies. Even when she reads the policies, she is still not aware of\nthe actual implications of the legal description. In the absence of\nother solutions, she does not read the policies and does not\nunderstand the\u00a0ramifications.JRC P3P Proxy Version 2.0 is a P3P user agent acting like an\nintermediary. Depending on the specified privacy preferences of a\nuser, it controls the access to web servers. Another known P3P user\nagent is AT&T Privacy Bird. Privacy Bird is a tool warning users if\nprivacy policies of visited websites are not matching with their\nindividual privacy\u00a0preferences.", "tags": "", "title": "Privacy-Aware Network Client"},{"url": "/patterns/Single-Point-of-Contact", "text": "Personal\u00a0AgentMany controllers make use of a storage platform (i.e. 'cloud' facilities), such as e-Health services that keep their sensitive patient data in a distributed online storage. The sensitivity of this information raises concern and garners a need for special care. The storage medium in this case rules out typical security\u00a0approaches.Effective distributed storage services require specialized privacy management. The deficiencies of traditional means may be expressed through the following:\n- traditional security mechanisms are platform dependent;\n- typically they are difficult to federate or distribute;\n- compliance with protocol can be cumbersome; and\n- as such they are often\u00a0inflexible.Single Point of Contact\u00a0adopts a claim-based approach for both authentication and authorization similar to a super-peer design, also acting as a (Resource) Security Token Service, an Identity and Attribute Provider, and a Relying Party. It features a tried and proven\u00a0expressive e-consent\u00a0language, and can communicate with other SPoCs in a Circle of\u00a0TrustA SPoC is essentially a security authority, which protects patients' privacy in e-Health applications by providing a claim-based authentication and authorisation functionality  (Baier et al. 2010), and facilitating secure communication between an e-Health service and its\u00a0clients.SPoC shares characteristics with a Central Medical Registry (CMReg), which performs authentication and manages identifying access to anonymised medical documents in a central repository. SPoC additionally facilitates secure e-Health service development and integration. It is able to share Electronic Health Records (EHRs) through a peer-to-peer network as an overarching, claim-based, super-peer-like representative of the e-Health community. Multiple SPoCs may also communicate, constituting a Circle of\u00a0Trust.See Fan et al. (2012) Figure 1 for a visual\u00a0depiction.A SPoC is able to issue security tokens as a Security Token Service (STS), authenticate local domain users as an Identity Provider, certify attributes as an Attribute Provider, and accept external claims as a Relying Party. When in a Circle of Trust, the SPoC can also translate the claims of other SPoCs as a Resource\u00a0STS.SPoCs' implementation of e-consent features the following levels, based on Coiera et al. (2004):\n- general consent [with or without specific exclusions];\n- general denial [with or without specific consents];\n- service authorisation;\n- service subscription; and\n-\u00a0investigation.For more information see Fan et al.\u00a0(2012).The SPoC ensures that the privacy of sensitive medical data is protected, and that it is distributed securely and only to the people who are allowed to access the data. However, it requires a reliable credential-based authentication system to be able to validate\u00a0requests.", "tags": "", "title": "Single Point of Contact"},{"url": "/patterns/Ambient-notice", "text": "Provide an ambient notice (unobtrusive, non-modal) when location is being accessed to increase awareness of ongoing\u00a0tracking.Provide unobtrusive, ongoing notice of real-time location\u00a0tracking.How can a service effectively provide notice to a user that location tracking is continuing, or happening again, without being as obtrusive as a repeated modal dialog? A user may not realize that an application given permission to access location is doing so continuously or repeatedly, or may not remember that explicit permissions given in the past allow a service to access data again later. In some cases, past explicit permission may not have been provided by the current user of the device (but instead by a spouse, parent or even an ex-spouse or stalker who temporarily had control of the device or the account). If notice is provided only at the time of consent, a user may inadvertently distribute personal information over a long period of time after having lost control of their device only\u00a0momentarily.A tray full of ambient notices may annoy or confuse users and inure them to ongoing practices. Take measures to avoid unnecessary notice. This must be balanced against the\nconcerns of an attacker's opting the user in without their\u00a0knowledge.Mac OS X Lion adds an ambient location services icon (a compass arrow) which appears in the task bar momentarily when an application is accessing the device's\u00a0location.Chrome adds a cross-hair icon to the location bar when a web site accesses the device location via the W3C Geolocation API. Clicking on the icon provides potential actions: clearing the saved consent for this site and accessing\u00a0settings.Similar examples exist in at least Android, iOS and\u00a0Win7.", "tags": "", "title": "Ambient notice"},{"url": "/patterns/Pseudonymous-messaging", "text": "A messaging service is enhanced by using a trusted third party to\nexchange the identifiers of the communication partners by\u00a0pseudonyms.This pattern can be used for online communications by email, through\nmessage boards, and\u00a0newsgroups.Messaging includes all forms of communication through emails,\narticles, message boards, newsgroups etc. This information could be\nstored and used to build sophisticated user profiles. Sometimes it can\nalso be used to prosecute\u00a0people.A message is send by a user to the server, which exchanges the\nsender's address with a pseudonym. Replied messages are sent back to\nthe pseudonymous address, which will then be swapped back to the\u00a0original.The goal of this pattern is to prevent unforeseen ramifications of the\nuse of online messaging\u00a0services.Users can communicate more freely. Pseudonym servers can be misused to\nsend offensive messages, for spam mails or by criminals for illegal\nactivities. Under those circumstances it could be necessary to revoke\nthe pseudonymity of the corresponding\u00a0parties.Alice is a political activist and tries to organize a political\ndemonstration. Since her government does not like free speech, her\ncommunication channels are intensely monitored and one day, she simply\ndisappears into a labor camp and is never seen\u00a0again.nym.alias.net a pseudonymous email system with the goal to provide\nsecure concealment of the user's identity. A Type I Anonymous Remailer\nforwards emails by modifying the message header and removing\nsender-related\u00a0information.", "tags": "", "title": "Pseudonymous Messaging"},{"url": "/patterns/Obligation-management", "text": "The pattern allows obligations relating to data sharing, storing and\nprocessing to be transferred and managed when the data is shared\nbetween multiple\u00a0parties.The developer aims to make sure that multiple parties are aware of and\ncomply with required user/organisational policies as personal and\nsensitive data are successively shared between a series of parties who\nstore or process that\u00a0data.Data may be accessed or handled by multiple parties that share data\nwith an organisation in ways that may not be approved by the data\u00a0subject.Service providers use an obligation management system. Obligation\nmanagement handles information lifecycle management based on\nindividual preferences and organisational policies. The obligation\nmanagement system manipulates data over time, ensuring data\nminimization, deletion and notifications to data\u00a0subjects.Benefits: privacy preferences and policies are communicated and adhered\nto among organisations sharing data. Liabilities: additional effort to\nset\u00a0obligations.A service provider subcontracts services, but requires that the data\nto be deleted after a certain time and that the service provider\nrequires to be notified if there is further\u00a0subcontracting.Pretschner et al (2009) provide a framework for evaluating whether a\nsupplier is meeting customer data protection obligations in\ndistributed systems. Researchers at IBM propose Enterprise Privacy\nAuthorization Language (EPAL) (2004) to govern data handling practices\naccording to fine-grained access control. Casassa Mont (2004) discusses\nvarious important aspects and technical approaches to deal with\nprivacy obligations. Pretschner, A., Schtz, F., Schaefer, C., and\nWalter, T.: Policy Evolution in Distributed Usage Control. Electron.\nNotes Theor. Comput. Sci. 244, 2009 IBM, The Enterprise Privacy\nAuthorization Language (EPAL), EPAL specification,\nhttp://www.zurich.ibm.com/security/enterprise-privacy/epal/, 2004\nMont, M. C., Dealing with Privacy Obligations, Important Aspects and\nTechnical Approaches, TrustBus,\u00a02004", "tags": "", "title": "Obligation Management"},{"url": "/patterns/Decoupling-[content]-and-location-information-visibility", "text": "Suggested: Retroactive Location\u00a0SharingThe organization in question (likely the controller) does not wish to undermine these expectations, and seeks to enable the user to assign contextually specific privacy\u00a0settings.Concerns about disclosing location information conflict with the appeal of location information for [content]\u00a0organization.Allow users to retroactively decide upon the disclosure of location information with respect to the context of the particular content. Record location information by default, but do not automatically share\u00a0it.Give users an interface or control to configure an access policy regarding the privacy of location information. That is, a place where users may, granularly or in bulk, define who may access location information of their\u00a0content.A basic solution could feature an interface or control for selecting the allowed users from all the types of users of the socially oriented service (e.g. built-in or user-defined groups, individuals, or anonymous users). This control could apply to individual content, or to multiple selections, or\u00a0groups.If a user chooses, certain individuals or groups may have default access to the attached location information. Default access like this, however, invalidates the following\u00a0approach.An extended solution may aim to be further privacy\u00a0preserving.The service may accept ciphertext as the location coupled with the content. When (and only if) the user chooses to make that specific location accessible, their client-side device decrypts the location and provides the service with the plaintext\u00a0location.By applying this pattern the controller prevents location access by default, and thus risks a low location sharing rate. This is due to the tendency of users to leave settings in the default state. However, depending on the effort, the controller may encourage positive public image, and raise adoption\u00a0overall.Based\u00a0on:S. Ahern, D. Eckles, N. Good, S. King, M. Naaman, and R. Nair, \u201cOver-Exposed ? Privacy Patterns and Considerations in Online and Mobile Photo Sharing,\u201d CHI \u201907, pp. 357\u2013366,\u00a02007.", "tags": "", "title": "Decoupling [content] and location information visibility"},{"url": "/patterns/attribute-based-credentials", "text": "Attribute Based Credentials (ABC) are a form of authentication mechanism that allows to flexibly and selectively authenticate different attributes about an entity without revealing additional information about the entity (zero-knowledge\u00a0property).ABC can be used in a variety of systems including Internet and smart\u00a0cards.Authentication of attributes classically requires full and unique authentication of an entity. For example, attributes (like age) could be put into a certificate together with name of the user, email address, public key, and other data about that entity. To corroborate an attribute (for example, that the user is an adult) the certificate has to be presented and all information have to be revealed. This is not considered a privacy-preserving\u00a0solution.There are multiple schemes to realize ABCs and implementations are also available. They typically all include a managing entity that entitles issuers to issue credentials to entities that could then act as provers of certain facts about the credentials towards\u00a0verifiers.A formal model can be found\u00a0here.To allow a user to selectively prove specific attributes like age > 18 to a verifying party without revealing any additional\u00a0information.ABC schemes require substantial compute power or optimization, so implementation may not be straightforward. Some projects like IRMA developed at Radboud University Nijmegen have shown that even resource restricted devices like smartcards can implement\u00a0ABCs.You want to issue an ID card that holds a users birthdate bd and can be used to prove that the card holder is old enough to view age-restricted movies in a cinema. Depending on the rating of the movie (minimum age x), the card holder can run a proof\u00a0that:Multiple uses of the card at the same cinema should not be\u00a0linkable.The most popular implementations\u00a0include:", "tags": "", "title": "Attribute Based Credentials"},{"url": "/patterns/Obtaining-Explicit-Consent", "text": "Explicit Consent / Obtaining Explicit consent via privacy agreement / Permission to Use Sensitive Data via Privacy Agreement - Explicit\u00a0ConsentIn order to offer services (or products) to users (data subjects), controllers often need to collect (process) user data. Sometimes this is sensitive, identifying, or just metadata or other information which may be correlated to become more invasive. This nonetheless enables them to offer competitive features and\u00a0functionality.However, controllers are required to obtain unambiguous consent from their users in order to process their personal data in any way. Depending on the legal jurisdiction, there are additional considerations to take into account depending on the type of data in question. Typically, sensitive data requires especially rigorous\u00a0care.Controllers which aim to make use of user data, especially that which can be used to identify the user or sensitive aspects about the user, may not do so without a legally binding and sound acquisition of the user's\u00a0consent.The controller must ensure each user's sufficient understanding of the potential consequences. Otherwise the consent might not be informed. They must verify their users' willingness despite those consequences to provide their data for the specific purposes they need. If they do not, the consent might not be freely\u00a0given.Ensuring that users do not consent based on time constraints, or the intimidation of the information provided, may require testing with a sample. If the sample is representative, it will give the controller a defense against any claims of\u00a0coercion.The mechanism used for users to signify their consent should be clear. For example, if it is a button, it could read \"I\u00a0consent.\"Controllers can derive clearer potential consequences when the data collected is the same for every consenting user. Users therefore can look over these risks and spend less time making a valid decision. This reduces the chances of users consenting without informing themselves due to the difficult or verbose content\u00a0presented.Based\u00a0on:J. Porekar, A. Jerman-Bla\u017ei\u010d, and T. Klobu\u010dar, \u201cTowards organizational privacy patterns,\u201d Proceedings - The 2nd International Conference on the Digital Society, ICDS 2008,\u00a02008.C. Bier and E. Krempel, \u201cCommon Privacy Patterns in Video Surveillance and Smart Energy,\u201d in ICCCT-2012, 2012, pp.\u00a0610\u2013615.Y. Asnar et al., \u201cInitial Set of Security and Privacy Patterns at Organizational Level,\u201d no. December 2006,\u00a02007.", "tags": "", "title": "Obtaining Explicit Consent"},{"url": "/patterns/Personal-data-store", "text": "Subjects keep control on their personal data that are stored on a\npersonal\u00a0device.The pattern is applicable to any data produced by the data subject (or\noriginally under his control) as opposed to data about him produced by\nthird\u00a0parties.Data subjects actually lose control over their data when they are\nstored on a server operated by a third\u00a0party.A solution consists in combining a central server and secure personal\ntokens. Personal tokens, which can take the form of USB keys, embed a\ndatabase system, a local web server and a certificate for their\nauthentication by the central server. Data subjects can decide on the\nstatus of their data and, depending on their level of sensitivity,\nchoose to record them exclusively on their personal token or to have\nthem replicated on the central server. Replication on the central\nserver is useful to enhance sustainability and to allow designated\nthird parties (e.g. health professionals) to get access to the\u00a0data.Enhance the control of the subjects on their personal\u00a0data.Data subjects need to be equipped with a personal data\u00a0store.Patients want to keep control over their health data but also to grant\nspecific access to some health\u00a0professionals.It has even been deployed for certain types of services, in\nparticular, in the health\u00a0sector.", "tags": "", "title": "Personal Data Store"},{"url": "/patterns/Federated-privacy-impact-assessment", "text": "The impact of personal information in a federation is more than the\nimpact in the\u00a0federatedIdentity Management scenarios (that is, when the roles of the Identity\nProvider and the Service Provider are\u00a0separated).Identity Management solutions were introduced to decouple the\nfunctions related to authentication, authorization, and management of\nuser attributes, on the one hand, and service provision on the other\nhand. Federated Identity Management allows storing a data subject's\nidentity across different systems. All together, these form a\nFederation that involves complex data\u00a0flows.Federated Management solutions can be used to improve privacy (e.g. by\nallowing service providers to offer their services without knowing the\nidentity of their users). However, the complexity of data flows and\nthe possibility of collusion between different parties entail new\nrisks and threats regarding personal\u00a0data.A Privacy Impact Assessment is conducted by all the members of the\nfederation, both individually and in conjunction, so as to define\nshared privacy policies, prove they are met, and demonstrate the\nsuitability of the architecture, in the benefit of all the\u00a0members.Deal with privacy risks associated from the federation of different\nparties in an Identity Management\u00a0solution.The consequences depend on the results of the privacy-impact\u00a0analysis.An Identity Provider issues pseudonyms to authenticate users at\nthird-party Service Providers, which can in turn check the\nauthenticity of these pseudonyms at the Identity Provider, without\ngetting to know the real user identity. However, the Identity Provider\nknows all the services requested by the users, which discloses\npersonal information to the Identity Provider and allows it to profile\nthe\u00a0users.", "tags": "", "title": "Federated Privacy Impact Assessment"},{"url": "/patterns/Data-breach-notification-pattern", "text": "This pattern assures that a certain minimum data breach notification delay is not\u00a0exceeded.This pattern is applicable in any environment where PII is stored and that allows monitoring of specific\u00a0events.In case a data breach has occurred, i.e. Personally Identifiable Information (PII) has leaked, the data owner must be notified. The notification process in turn may not work correctly, so it has to be\u00a0monitored.A monitoring system logs access to clients' PII along with a time-stamp. A notification process continuously verifies that only authorized access is listed in this log file, and in case of unauthorized access notifies the data owner and logs the notification action in the log file, again accompanied by a time-stamp. A notification monitoring process finally continuously checks that t_n - t_l <= max_np (t_n denoting the time of notification, t_l the time of data leakage, max_np the maximally allowed period of notification). In case t_n - t_l > max_np it alerts the PII Incident\u00a0Manager.The pattern goal is to constantly ensure a minimum delay of notification, should a data breach have occurred, and in case a notification exceeds the allowed delay, to indicate this by appropriate\u00a0means.In order to use the pattern, the personal data processor must have in place an access control mechanism and a monitoring mechanism that allows to monitor every PII access. The pattern cannot ensure that a PII Incident Manager will take adequate actions, hence this process has to be established and controlled by other\u00a0means.Assume a company Comp stores all employees' data with a storage service Store. There is a contractual agreement between Comp and Store that each data leakage is reported within one hour. Now Bob, an employee of Store and not authorized to read Comp's data, succeeds in circumventing Store's access control mechanisms and reads Store's data. This represents a data breach of which Comp has to be notified within an\u00a0hour.This pattern is based on the privacy principle \"Accountability\" specified in ISO/IEC 29100 that is also used in Annex A of ISO/IEC 27018. More specifically, it addresses A.9.1 Notification of a data breach involving Personally Identifiable Information (PII). Uses of the pattern as a concrete instantiation of A.9.1 are not\u00a0known.", "tags": "", "title": "Data Breach Notification Pattern"},{"url": "/patterns/Sticky-policy", "text": "Machine-readable policies are sticked to data to define allowed usage\nand obligations as it travels across multiple parties, enabling users\nto improve control over their personal\u00a0information.Multiple parties are aware of and act according to a certain policy\nwhen privacy-sensitive data is passed along the multiple successive\nparties storing, processing and sharing that\u00a0data.Data may be accessed or handled by multiple parties that share data\nwith an organisation in ways that may not be approved by the data\u00a0subject.Service providers use an obligation management system. Obligation\nmanagement handles information lifecycle management based on\nindividual preferences and organisational policies. The obligation\nmanagement system manipulates data over time, ensuring data\nminimization, deletion and notifications to data\u00a0subjects.The goal of the pattern is to enable users to allow users to control\naccess to their personal\u00a0information.Bene\ufb01ts: Policies can be propagated throughout the cloud to trusted\norganisations, strong enforcement of the policies, traceability.\nLiabilities: Scalability: policies increase size of data. Practicality\nmay not be compatible with existing systems. It may be difficult to\nupdate the policy after sharing of the data and existence of multiple\ncopies of data. It requires ensuring data is handled according to\npolicy e.g. using\u00a0auditing.When data is shared by an organisation they can use privacy preserving\npolicy to enforce respecting user privacy by third party organisations\nthat use, process and store such data. For example, a hospital may\nshare data with third party organisations requiring adhering to\nspecific privacy policies associated with the\u00a0data.", "tags": "", "title": "Sticky Policies"},{"url": "/patterns/TEST-patterns-stub", "text": "This is a test pattern - used for rendering tests. Please\u00a0ignorelorem\u00a0ipsum", "tags": "", "title": "THIS IS A TEST PATTERN - STUB"},{"url": "/patterns/Location-granularity", "text": "Support minimization of data collection and distribution. Important when a service is collecting location data from or about a user, or transmitting location data about a user to a\u00a0third-party.When a service is collecting location data from or about a user, or transmitting location data about a user to a\u00a0third-party.Many location-based services collect current or ongoing location information from a user in order to provide some contextual service (nearest coffee shop; local weather; etc.). Collecting more information than is necessary can harm the user's privacy and increase the risk for the service (in the case of a security breach, for example), but location data may still need to be collected to provide the service. Similarly, users may want the advantages of sharing their location from your service to friends or to some other service, but sharing very precise information provides a much greater risk to users (of re-identification, stalking, physical intrusion,\u00a0etc.).Accepting or transmitting location data at different levels of granularity generally requires a location hierarchy or geographic ontology agreed upon by both services and a more complex data storage model than simple digital\u00a0coordinates.  Truncating latitude and longitude coordinates to a certain number of decimal places may decrease precision, but is generally not considered a good fuzzing algorithm. (For example, if a user is moving in a straight line and regularly updating their location, truncated location information will occasionally reveal precise location when the user crosses a lat/lon boundary.) Similarly, using \"town\" rather than lat/lon may occasionally reveal more precise data than expected when the user crosses a border between two\u00a0towns.Fire Eagle specifically requires that recipient applications be written to handle data at any of the levels, and allows updating the user's location at any level of\u00a0granularity.One of the fore-runners to the W3C Geolocation API, Firefox's experimental Geode feature allowed JavaScript access to the current location at four different levels of\u00a0granularity.", "tags": "", "title": "Location Granularity"},{"url": "/patterns/Incentivized-Participation", "text": "Reciprocity / Fair distribution of efforts / Win-win situation /\u00a0Pay-BackUsers of a system have varying privacy concerns, and different sensitivities associated with their personal information. These users need ways to contribute without leaking sensitive details, or to perceive a worthwhile tradeoff for those details. This can be achieved through social encouragement (i.e. participation and shared trust), direct value exchanges (discounts and giveaways), or some other derived value (e.g. positive\u00a0reinforcement).A data controller derives various values from the participation of its users (i.e. data subjects). The more that these users participate, explicitly providing context and implicitly providing metadata (e.g. statistics and telemetry), the better the controller fares in a number of respects. Despite this key relation, over-sharing can greatly infringe upon a user's right to privacy. Many controllers therefore aim to respect this right when benefiting from user\u00a0interactions.Controllers which gain from user activity want to push for participation, but this can negatively affect\u00a0users.Users have varying degrees of concern about their privacy, and do not respond to different forms of encouragement the same way. By penalising under-sharing and inactivity, or being misleading, users become alienated and distrusting of the system. As such this problem has multiple elements. These include asymmetric returns on investment, and the standard incentive deficiency, where users lack the encouragement to\u00a0participate.In many situations, some benefit more than others. In extreme cases, users may benefit through minimal participation and thus contribute very little to the system's derived value. Those who do not perceive an acceptable value despite considerable contribution may then\u00a0withdraw.An example of this behaviour might be seen in dating sites where users with only a flattering picture may succeed more than those with detailed profiles. Similar cases can be made for other social media, as well as with asymmetric bandwidth on peer to peer sharing. With torrent technologies, this is often referred to as\u00a0'leeching'.Users which provide limited or vague information due to privacy concerns may have less opportunity for participation. Another way this occurs is when they are not driven by positive social reinforcement. The lack of friends, followers, potential matches, etc. leads to user\u00a0inactivity.Privacy concerns need to be met with valid reassurances about issues which matter to the user. Firstly, users should know that the system holds their preferences in high regard. Secondly, they should perceive real value in their participation. Finally, if desired, users should be assisted in a smooth transition into the\u00a0ecosystem.The three elements of the solution are elaborated on in the following\u00a0sections.Users need to know they are able to participate without the system undermining their personal preferences. This should apply from the very first usage of a system. Everything the system does globally must adhere to privacy friendly defaults. Any service which cannot uphold these expectations should be deactivated for new users, and only be enabled once these users consent to the additional processing. Attempts to solicit this should not be\u00a0invasive.With privacy concerns at ease, encouraging equal participation entails reciprocity. These can be in both social and financial\u00a0forms.All participation should result in value derivation (social or otherwise) for all participants, and not just individuals. As a consequence of this mindset, the derived benefits of users who do not participate are limited. This secondary effect may also be the primary mechanism for reciprocity, though positive sum approaches will be met with more\u00a0support.As an additional measure, or where equality is not feasible, provide an alternative incentive. If not social, then financial incentives (discounts, waived fees) can be provided to active users. This can be limited to those who the system can identify as receiving poor value returns on their contributions. User retention examples tend to be less frequent, with the odd website sending 'come back' emails which promise fee\u00a0reductions.Another approach is the explicit provision of virtual currency necessary to benefit from the system, those who contribute will then have more currency at their disposal. They may opt to be applauded for their efforts publicly, but again should not be forced\u00a0to.Examples of social value perception are the Facebook like/reaction, Google's +1, Reddit Gold, and Twitter's reposting. These approaches are enacted by the users instead of the system, and are therefore less\u00a0intrusive.In terms of computer aided pairing, greater participation may be achieved if users consent to intelligent nudging. Sh\u00fcmmer suggests that users are more likely to participate when their interests are shared with others, and thus, a system would help users identify those with similar interests. This is another solution which relies on prior consent, however. By encouraging interaction between these users, a system would derive more activity and therefore further\u00a0value.On the other hand, Sh\u00fcmmer points out that mixing dissimilar users may also result in unexpected activity. It may allow the system to discover notions about its users which were not previously apparent. This is yet another way to increase value, though will likely be far more intrusive than the former. Users should be properly informed of the possible consequences of 'mixing it\u00a0up'.In order to ensure that recommendations made by the system do not have increasingly negative side effects, the system should learn from ineffective suggestions. This is limited to where it has permission to do so. Where user activity drops, a system should aim not continue in the same fashion as before. When it climbs, however, the system should improve whatever characteristics likely resulted in that\u00a0climb.This can be made more explicit by soliciting feedback from the user themselves, as also suggested by Sh\u00fcmmer. Even this, though, is subject to negative reactions. However, acclimating users to an environment of openness and transparency will also build trust - potentially resulting in the use of services users would not have used\u00a0otherwise.Applying the concepts represented in this pattern may have certain trade-offs\u00a0associated.Isolating users, and learning from their actions, based on feedback loops requires prior informed and explicit consent, as potentially invasive conclusions may be\u00a0derivedPermission restricted Buddy Lists in Instant Messaging, or more extensive social networks; Allowing users to filter their sharing by access groups which they\u00a0define.Examples of Value\u00a0PerceptionFacebook like/reaction, Google's +1, Reddit Gold, and Twitter's\u00a0repostingExamples of Transition\u00a0AssistanceT. Sh\u00fcmmer, \u201cThe Public Privacy \u2013 Patterns for Filtering Personal Information in Collaborative Systems,\u201d in Proceedings of CHI workshop on HCHI Patterns,\u00a02004.", "tags": "", "title": "Incentivized Participation"},{"url": "/patterns/Strip-invisible-metadata", "text": "Strip potentially sensitive metadata that isn't directly visible to the end\u00a0user.When a service requires a user to import data from external sources (eg.\npictures, tweets, documents) different types of metadata may be \ntransmitted. Users may not be aware of the metadata as it can be\nautomatically generated or not directly visible. Services might be\ninadvertently responsible for exposing private metadata, or going\nagainst users'\u00a0expectations.Users are not always fully aware of the various kinds of metadata\nattached to files and web resources they share with online services.\nMuch of this data is automatically generated, or not directly visible to\nusers during their interactions. This can create situations where, even\nthough users share information explicitly with services, they may be\nsurprised to find this data being revealed. In certain cases where the\ndata is legally protected, the service could be held responsible for any\nleakage of sensitive\u00a0information. How should services that need users to share data and upload files\ntreat additional metadata attached with files? In case of uploading\ndocuments and images, which parts of the metadata can be treated as\nexplicitly shared\u00a0information.Stripping all metadata that is not directly visible during upload time,\nor during the use of the service can help protect services from\nleaks and liabilities. Even in cases where the information is not\nlegally protected, the service can protect themselves from surprising\ntheir users and thus alienating\u00a0them. To summarize: user metadata that can not be made visible to users\nclearly should be stripped to avoid overstepping the users'\u00a0expectations. Twitter.com removes EXIF data from images uploaded to their image\n   sharing service. Previously there have been many breaches of personal\n   location by using EXIF data shared by image sharing\u00a0services. In certain cases services might build features based on\n   metadata, or the metadata sharing could be an important part of the\n   community of users. Flickr.com allows users to hide their EXIF data from\n   public display, and also provides an interface for users to easily see\n   whether they are sharing location as part of uploading their\u00a0images. TODO: add\u00a0screenshots", "tags": "", "title": "Strip Invisible Metadata"},{"url": "/patterns/Aggregation-gateway", "text": "Encrypt, aggregate and decrypt at different\u00a0places.A service provider gets continuous measurements of a service attribute linked to a set of individual service\u00a0users.The provision of a service may require detailed measurements of a service attribute linked to a data subject to adapt the service operation at each moment according to the demand load. However, these measurements may reveal further information (e.g. personal habits, etc.) when repeated over\u00a0time.A feeder metering system can be added as a measuring rod which introduces a comparison for each group of\u00a0meters.Let the service provider have reliable access to the aggregated load at every moment, so as to fulfil its operating requirements, without letting it access the individual load required from each specific service\u00a0user.There is a need to deploy trusted third parties that compute the aggregations over each group of users. Note that they need to be honest (i.e., they cannot collude with the other parties involved), but they need not respect confidentiality (as they only have access to encrypted contents). Smart meters are needed that have computation resources to apply secret generation and homomorphic encryption procedures (note that this is trivial when dealing with the use of computational resources, but it does not have to be always available in the case of e.g. smart grid systems). The potential range of measured values must be large enough to avoid brute force attacks. Robust homomorphic encryption schemes introduce a large computational\u00a0load.An electric utility operates a smart grid network with smart meters that provide measurements of the instantaneous power consumption of each user. The utility employs that information to adapt the power distribution in a dynamic fashion, according to the user demand at each\u00a0moment.", "tags": "", "title": "Aggregation Gateway"},{"url": "/patterns/Privacy-color-coding", "text": "In a social networking site a user gets direct visual cues which\nprivacy settings apply on which shared\u00a0elements.The pattern can be used in applications where users share and publish\npersonal data and contents, but can control their visibility using\nprivacy settings. This includes but is not limited to social\nnetworking\u00a0sites.Privacy settings and the actual effect of these settings on shared\ncontent and data is often not obvious for the user. Not having the\nactive settings constantly in mind might lead to non-optimal privacy\nexperiences when the perceived privacy settings differ from the actual\u00a0settings.The results of privacy settings such as visibility are divided into\ndifferent levels. A distinct color is assigned to each of these\nlevels. Every time the user is performing an action where privacy\nsettings come into play, the color is used as an indication of the\nprivacy settings currently in effect. The choice of colors should take\ninto account prevalent color meanings, like usage of the color red for\nwarning situations. If privacy settings cannot be grouped into\ndistinct levels, a gradient between different colors could also be\u00a0used.Users receive direct visual cues on the consequences of their privacy\nsettings currently in effect. In order to be more clear about their\nprivacy\u00a0settings.Users will directly see the outcome of their privacy settings. The\ndanger of unwanted actions is decreased, as users will permanently\nreceive visual cues. On the other hand a reduction of complex settings\nto a few colors may lead to an oversimplification which would render\nthe whole pattern useless. Visual cues must be integrated into the\nsite design but must still be placed prominently enough to be\nnoticeable. Cultural aspects for the different meanings of colors\nshould be taken into account. The same color may not be recognized as\na warning label in different\u00a0cultures.Alice uses a social network and shares personal stories only with her\nfriends while she shares mundane content publicly. Hence she always\nhas to change the privacy settings of her posts in order to adjust the\nvisibility of the posts. One day she forgets to change the setting and\ndoes not realize that she actually shared a precarious story with her\u00a0boss.", "tags": "", "title": "Privacy Color Coding"},{"url": "/patterns/Masquerade", "text": "Anonymous\u00a0InteractionUsers act differently under active supervision, and this may negatively impact their content\u00a0generation.Allow users to select their desired identifiability for the context in question. They may reveal some subset of the interaction or account attributes and filter out the\u00a0rest.For implementing this pattern, a configuration interface will be required. Two approaches could be considered: levels of publicity or publicity\u00a0profiles.In levels of publicity, all possibly revealed information could be arranged on a scale depending on how identifying each kind of information is alone or when shown together. A visual element could be used to select a specific publicity level. When the users select one level, all information with the same or smaller publicity level will be revealed. This is taken into account when measuring where upon the scale a piece of information\u00a0falls.In publicity profiles, all possibly revealed information could be depicted using visual elements and the users have to select each kind of information that they want to reveal. Furthermore, depending on the kind of information, the users could define different granularity for each one (E.g. regarding location it is possible to define the country, region, city, department and so\u00a0on).Based\u00a0on:T. Sch\u00fcmmer, \u201cThe Public Privacy \u2013 Patterns for Filtering Personal Information in Collaborative Systems,\u201d in Proceedings of CHI workshop on Human-Computer-Human-Interaction Patterns,\u00a02004.", "tags": "", "title": "Masquerade"},{"url": "/patterns/Active-broadcast-of-presence", "text": "Controllers provide an interface for acquiring information about the user. When one such user wants to share or broadcast their information, such as location or other presence data, that user may want to constrain the information. In this way, they may wish to prioritize data that is contextually relevant, or avoid a full stream of data which may be either noisy or intrusive. The controller wants the user to be able to provide this data at will, to maximize the applicability of their services. However, they do not want the user to regret providing too much data, nor to bother the user with constant\u00a0requests.A service aims to acquire or broadcast a user's real-time data, particularly presence or location information, to a platform (e.g. social network). They wish to do so without revealing sensitive data (e.g. private locations, histories, or health information) nor overwhelming recipients with noisy data or users with constant\u00a0requests.The service may present distinct contexts in which to honor explicit settings, but in absence of this context assume that further consent is required. The user may choose not be be asked again, but must make this decision\u00a0explicit.In addition to privacy settings with appropriate defaults, allow the user the option to be asked again, every time the context\u00a0changes.By default, users should actively choose to broadcast rather than the service deciding based on general settings which may not apply to the present context. Various contexts may be provided distinct\u00a0settings.In these situations users need only be reminded prior to setting the values themselves. After this, they may choose to be notified about broadcasting, but not about sharing with the service itself. In this way, the user may decide\u00a0later.Based\u00a0on:", "tags": "", "title": "Active broadcast of presence"},{"url": "/patterns/Outsourcing-[with-consent]", "text": "Controllers often do not have the means to feasibly or sufficiently process the data they oversee to the extent they desire. In these cases they seek an external processor or third party to handle the process. This typically conflicts with their already obtained consent from their users (their data subjects), as further processing by a third party is not necessarily compatible with the agreed upon purposes. In these situations the controller does not have legally obtained consent for this processing and will be liable if they carry it\u00a0out.Third party processors do not inherent user consent granted to a controller, but need each user's consent before they may process their information. The processor cannot contact the necessary users as they have no lawful access to any means to identify\u00a0them.Obtain additional (Lawful Consent)[Lawful-Consent] for the specific purposes needed from each user before allowing the third party to process their data. Do not process the data of users who do not\u00a0consent.The consent can be seen as a contract establishing what and how data will be processed by the [third party]. The [controller] must also ensure, preferably by a written agreement, that the [third party] strictly follows all conditions relating to data processing that were imposed on\u00a0[them].Figure 2(b) shows an SI* model explaining the solution of Compagna et al.\u00a0(2007)The pattern solves the problem of granting [the consent] necessary to perform out-sourced data processing by assuring [users that their information is] processed according to the\u00a0contract.The scenario described by Compagna et al. (2007) features a Health Care Centre (data controller) and a user (data subject), Bob, who needs constant supervision. The subcontractor, a Sensor Network Provider (third party supplier), installs and maintains the network responsible for automated monitoring of Bob's health. This subcontractor needs additional specific, informed, explicit, and freely given consent from\u00a0Bob.L. Compagna, P. El Khoury, F. Massacci, R. Thomas, and N. Zannone, \u201cHow to capture, model, and verify the knowledge of legal, security, and privacy experts : a pattern-based approach,\u201d ICAIL \u201907,\u00a02007.", "tags": "", "title": "Outsourcing [with consent]"},{"url": "/patterns/Pseudonymous-identity", "text": "Hide the identity by using a pseudonym and ensure a pseudonymous\nidentity that can not be linked with a real identity during online\u00a0interactions.This pattern can be used for systems in which users are identified by\npublic\u00a0identities.Many kinds of sensitive informations are released through web\ninteractions, email, data sharing or location-based systems, which can\ncontain the name of a user or header information in packets. Another\nproblem could be to interact anonymously in a forum. However too much\ninteraction in a forum with an anonymous identity can be dangerous in\nthe sense that the relation between original identity and a\npseudonymous identity can be\u00a0exposed.Initiate a random pseudonym, that can not be related to the original,\nso that the identity is hidden. Furthermore a pseudonym depends on\nconcealment, so the pseudonym allocation needs\u00a0protection.Hide the identity of the\u00a0participants.The real identity of a user is hidden. In certain scenarios there is a\nneed for additional space to store the pseudonym-identity mapping.\nExtensive Usage of the same pseudonym can weaken\u00a0it.Assuming some students are writing an exam and they have to fill out a\nform about their identity, where there is an optional field for a\nchosen pseudonym. This way the result can be released under the chosen\npseudonyms and the identity of each student is hidden. But by being\nobservant, some students might be able to figure out which identity\nbelongs to which pseudonym and so the confidentiality of the identity\nis\u00a0compromised.Anonymizer are well-known tools for anonymous web interactions. They\nwork for example by using a proxy between a request sender and a\nrecipient to strip header information like HTTP_USER_AGENT in packet\nheaders because they contain metadata about packet senders. The\nMixmaster is an anonymous remailer that hides the sender and recipient\nidentity by stripping its name and assigning a pseudonym. Some data\nsharing systems with a privacy-preserving focus make use of pseudonyms\nso that identifying information such as names and social security\nnumbers are hidden. For example various electronic healthcare systems\nare using pseudonyms for the storage of e-health\u00a0records.", "tags": "", "title": "Psuedonymous Identity"},{"url": "/patterns/Policy-matching-display", "text": "Give one careful thought to your privacy needs, then be always able to swiftly apply what you\u00a0decided.A user wants to start using a new service, which lets the user configure several privacy-related parameters. The user often does the same with new, different service\u00a0providers.Users may get overwhelmed by the burden of deciding on privacy aspects each and every time they enrol in a new service. This may make them err on their decisions regarding\u00a0privacy.Before contracting a service, the service provider retrieves the user preferences (exposed by their user agent, or at a well-known URI), and presents the user a comparison between their preferences and the privacy policies applied by default by the service operator, which in turn automatically adapts any configurable values to the user\u2019s declared\u00a0preferences.Allow users to provide a consistent privacy-related behaviour, while reducing their cognitive workload every time they enrol in a new\u00a0service.This pattern requires sharing a machine-readable format to express and exchange definitions of privacy policies between the user agent and the service providers. Several such formats exist, yet they are not always supported by either user agents or by service providers. Besides, not all the privacy policy nuances can be expressed in existing privacy policy\u00a0languages.", "tags": "", "title": "Policy Matching Display"},{"url": "/patterns/Anonymous-reputation-based-blacklisting", "text": "Get rid of troublemakers without even knowing who they\u00a0are.A service provider provides a service to users who access anonymously, and who may make bad use of the\u00a0service.Anonymity is a desirable property from the perspective of privacy. However, anonymity may foster misbehaviour, as users lack any fear of\u00a0retribution.A service provider can assign a reputation score to its users, based on their interactions with the service. Those who misbehave earn a bad reputation, and they are eventually added to a black list and banned from using the service anymore. However, these scoring systems traditionally require the user identity to be disclosed and linked to their reputation score, hence they conflict with anonymity. This has made, for instance, Wikipedia administrators to take the decision to ban edition requests coming from the TOR network, , as they cannot properly identify users who\u00a0misbehave.A Trusted Third Party (TTP) might be introduced in between the user and the service provider. The TTP can receive reputation scores from the service provider so as to enforce reputation-based access policies, while keeping the identity hidden from the service provider. However, this would require the user to trust the TTP not to be a whistle-blower\u00a0indeed.How can we make users accountable for their actions while keeping them\u00a0anonymous?First, the service provider provides their users with credentials for anonymous\u00a0authentication.Then, every time an authenticated user holds a session at the service, the service provider assigns and records a reputation value for that session, depending on the user behaviour during the session. Note that these reputation values can only be linked to a specific session, but not to a specific user (as they have authenticated\u00a0anonymously).When the user comes back and starts a new session at the service, the service provider challenges the user to prove in zero-knowledge that he is not linked to any of the offending sessions (those that have a negative reputation associated). Zero-knowledge proofs allow the user to prove this, without revealing their identity to the service provider. Different, alternative proofs have been proposed, e.g. prove that the user is not linked to any of the sessions in a set of session IDs, prove that the last K sessions of the user have good reputation,\u00a0etc.In practice, more complex blacklisting rules can be applied as well. For instance, several reputation scores can be assigned to the same session, each regarding different facets of the user behaviour. Then, the blacklisting thresholds may take the form of a Boolean combination or a lineal combination over individual session and facet reputation\u00a0values.A service provider wants to prevent users who misbehave from accessing the service anymore, without gaining access to their\u00a0identity.Different implementations may only be practical for services with a reduce number of users, require intense computations, limit the scope of the reputation to a constrained time frame, be vulnerable to Sybil attacks, etc. Nonetheless, protocols are being improved to overcome these and other issues. See the cited sources below for the specific\u00a0discussion.A wiki allows any visitor to modify its contents, even without having been authenticated. Some malicious visitors may vandalize the contents. This fact is signalled by the wiki administrators. If a visitor coming from the same IP address keeps vandalizing the site, they will earn a bad reputation, and their IP will be banned from modifying the contents anymore. However, users accessing through a Tor anonymity network proxy cannot be identified from their IPs, and thus their reputation cannot be\u00a0tracked.", "tags": "", "title": "Anonymous Reputation-based Blacklisting"},{"url": "/patterns/Asynchronous-notice", "text": "How can a service effectively provide notice to a user who gave permission once but whose information is accessed repeatedly (perhaps even continuously) over a long period of time? Proactively notify the user after the time of consent that information is being tracked, stored or\u00a0re-distributed.Support notice of ongoing location\u00a0tracking.How can a service effectively provide notice to a user who gave permission\nonce but whose information is accessed repeatedly (perhaps even continuously)\nover a long period of time? If a user forgets that they gave access (or who\nhas access) they may later be surprised or upset by the continued flow of\npersonal information. Also, initial consent may have been forged by an\nattacker or have been provided by another user of a shared device -- if\nsynchronous notice is only provided at the time of consent, a user may\ninadvertently distribute personal information over a long period of time after\nhaving lost control of their device only\u00a0momentarily.Providing an asynchronous notice requires a reliable mechanism to contact the\nuser (a verified email address or telephone number, for example). Care should\nbe taken to ensure that the mechanism can actually reach the person using the\ndevice being tracked. (For example, notifying the owner of the billing credit\ncard may not help the spouse whose location is being surreptitiously\u00a0tracked.)Many repeated notices may annoy users and eventually inure them to the\npractice altogether. Take measures to avoid unnecessary notices and some level\nof configuration for frequency of notices. This must be balanced against the\nconcerns of an attacker's opting the user in without their\u00a0knowledge.By ensuring that users aren't surprised, asynchronous notice may increase\ntrust in the service and comfort with continued disclosure of\u00a0information.Google Latitude users can configure a reminder email (see below) when their\n   location is being shared with any application, including internal applications\n   like the Location History\u00a0service.Google Location History lets you store your history and see a dashboard of interesting information such as frequently visited places and recent trips. \n   Google Talk Location Status lets you post your location in your chat status. \n   Google Public Location Badge lets you publish your location on your blog or\u00a0site. ", "tags": "", "title": "Asynchronous notice"},{"url": "/patterns/Privacy-dashboard", "text": "An informational privacy dashboard can provide collected summaries of the collected or processed personal data for a particular\u00a0user.Help users see an overview of the personal information collected about them, particularly when the data or services in question are\u00a0numerous.When your service collects, aggregates or processes personal information from users, particularly information that changes over time, is collected or aggregated in ways that might be unexpected, invisible or easily forgotten, or where users have options for access, correction and\u00a0deletion.How can a service succinctly and effectively communicate the kind and extent of potentially disparate data that has been collected or aggregated by a service? Users may not remember or realize what data a particular service or company has collected, and thus can't be confident that a service isn't collecting too much data. Users who aren't regularly and consistently made aware of what data a service has collected may be surprised or upset when they hear about the service's data collection practices in some other context. Without visibility into the actual data collected, users may not fully understand the abstract description of what types of data are collected; simultaneously, users may easily be overwhelmed by access to raw data without a good understanding of what that data\u00a0means.An informational privacy dashboard can provide collected summaries of the collected or processed personal data for a particular user. While access to raw data may be useful for some purposes, a dashboard provides a summary or highlight of important personal data. Seek to make the data meaningful to the user with examples, visualizations and\u00a0statistics.Where users have choices for deletion or correction of stored data, a dashboard view of collected data is an appropriate place for these controls (which users may be inspired to use on realizing the extent of their collected\u00a0data).In short, a dashboard answers the common user question \"what do you know about me?\" and does so in a way that the user can understand and take appropriate action if\u00a0necessary.Google Accounts: About the\u00a0Dashboard", "tags": "", "title": "Privacy dashboard"},{"url": "/patterns/Reasonable-Level-of-Control", "text": "Users have certain expectations about what level of privacy they can expect in certain contexts. In general, they are given the means to provide themselves with as much or little shielding from intrusions as they need. This expectation carries over to usage of services (or products) offered by a Controller. Users expect that they can have an impact on what about them is known to a service, or others that use the\u00a0service.Users expect to be afforded sufficient self-determination over what information about them is collected or otherwise processed. The level of information and control desired, however, varies from person to person, as does the negative response when expectations are not\u00a0met.Allow users to selectively and granularly provide information to a service, or its users, and have select information available to user-defined or predetermined\u00a0groups.Users should be able to push their chosen information to (or have it pulled by) those they grant access. Using push mechanisms, users will have the greatest level of control due to the fact that they can decide the privacy level of their data case by\u00a0case.Pull mechanisms are less granular, as granting access to a group or individual continues until that access is denied. Within this time frame, the sensitivity of the data may fluctuate. However, the user should have the ability to retract access at will, and thus, can manage their own identified\u00a0risks.Elsewhere, ensure that any required fields are truly required, and that the completeness needed for those fields be indicated. When there are automatic suggestions, let users redefine or remove the information before it is collected by the service. These automatic suggestions should also not take place without\u00a0consent.Where information is provided on a continual basis to those granted access, provide the user with the necessary tools. They should be able to indicate who falls within a group, and what exactly that group can access, for how long, at what granularity, how far back they can look, and so\u00a0forth.Based\u00a0on:", "tags": "", "title": "Reasonable Level of Control"},{"url": "/patterns/Anonymity-set", "text": "This pattern aggregates multiple entities into a set, such that they\ncannot be distinguished\u00a0anymore.This pattern is applicable in a messaging scenario, where an attacker\ncan track routing information. Another possible scenario would be the\nstorage of personal information in a\u00a0database.In a system with different users we have the problem that we can often\ndistinguish between them. This enables location tracking, analyzing\nthe behaviour of the users or other privacy-infringing\u00a0practices.There are multiple ways to apply this pattern. One possibility is, to\nstrip away any distinguishing features from the entities. If we do not\nhave enough entities, such that the anonymity set would be too small,\nthen we could even insert fake\u00a0identities.The goal of this pattern is to aggregate different entities into a\nset, such that distinguishing between them becomes\u00a0infeasible.One factor to keep in mind is that this pattern is useless if there\nare not many entities, such that the set of probable suspects is too\nsmall. What \"too small\" means depends on the exact scenario. Another\nfactor is a possible loss of\u00a0functionality.Assuming that there are two companies, one is a treatment clinic for\ncancer and the other one a laboratory for research. The Clinic\nreleases its Protected Health Information (PHI) about cancer victims\nto the laboratory. The PHI's consists of the patients' name, birth\ndate, sex, zip code and diagnostics record. The clinic releases the\ndatasets without the name of the patients, to protect their privacy. A\nmalicious worker at the laboratory for research wants to make use of\nthis information and recovers the names of the patients. The worker\ngoes to the city council of a certain area to get a voter list from\nthem. The two lists are matched for age, sex and location. The worker\nfinds the name and address information from the voter registration\ndata and the health information from the patient health\u00a0data.Anonymity sets are in use in various routing obfuscation mechanisms\nlike Onion Routing. Hordes is a multicast-based protocol that makes\nuse of multicast routing like point-to-multipoint delivery, so that\nanonymity is provided. Mix Zone is a location-aware application that\nanonymizes user identity by limiting the positions where users can be\u00a0located.", "tags": "", "title": "Anonymity Set"},{"url": "/patterns/Privacy-icons", "text": "A privacy policy which is hard to understand by general audience is summarized and translated into commonly agreed visual icons. A privacy icon is worth a thousand-word\u00a0policy.This pattern can be applied to any system which collects end user data. It can be presented in an interactive web page but also as part of a physical product which can collect data (e.g. fitness\u00a0tracker)Many organizations provide privacy policies which are too lengthy and hard to understand by the general audience. These policies are oriented as legal disclaimers for legal issues, rather than to inform end users so they can consent to the organization practices after being clearly informed of the collected data, its purpose, and the processing and potential sharing with third\u00a0parties.Include within the service/device a very accessible and visual explanation of the privacy policy. Icons are a great complement to written text, as they may convey much information at a glance through a different modality (images). Standardized icon sets may thus be added to the privacy\u00a0policy.Truly inform customers of the privacy policy of a\u00a0system/organizationUsers may understand, at first glance, what are the potential risks of consenting of a privacy policy. In order to be useful, the icons must be well known and understood by the majority of the potential users before being used. A common meaning of the icon needs to be shared by the community. Educational material can be built upon the implications of each of these\u00a0icons.Alice buys a fitness tracker and she is aware that the device collects her location, and sends it to a central web service in order to provide her with her fitness statistics (her fitness routes, the time spent...). The device provider aggregates this data and provides a business analytics service to third\u00a0parties.Alice is totally unaware of this secondary use of her data and may not agree to it. But accessing this policy involves accessing a website and going through a lengthy and legally oriented\u00a0document.Currently, most of these are only applied by client-side\u00a0solutions.See also the Privacy Icons entry at Ideas for a Better Internet (kind of a pattern repository by the Berkman Center for Internet and Society in\u00a0Harvard).", "tags": "", "title": "Privacy icons"},{"url": "/patterns/Attribute-based-credentials", "text": "Attribute Based Credentials (ABC) are a form of authentication mechanism that allows to flexibly and selectively authenticate different attributes about an entity without revealing additional information about the entity (zero-knowledge\u00a0property).ABC can be used in a variety of systems including Internet and smart\u00a0cards.Authentication of attributes classically requires full and unique authentication of an entity. For example, attributes (like age) could be put into a certificate together with name of the user, email address, public key, and other data about that entity. To corroborate an attribute (for example, that the user is an adult) the certificate has to be presented and all information have to be revealed. This is not considered a privacy-preserving\u00a0solution.There are multiple schemes to realize ABCs and implementations are also available. They typically all include a managing entity that entitles issuers to issue credentials to entities that could then act as provers of certain facts about the credentials towards\u00a0verifiers.A formal model can be found\u00a0here.To allow a user to selectively prove specific attributes like age > 18 to a verifying party without revealing any additional\u00a0information.ABC schemes require substantial compute power or optimization, so implementation may not be straightforward. Some projects like IRMA developed at Radboud University Nijmegen have shown that even resource restricted devices like smartcards can implement\u00a0ABCs.You want to issue an ID card that holds a users birthdate bd and can be used to prove that the card holder is old enough to view age-restricted movies in a cinema. Depending on the rating of the movie (minimum age x), the card holder can run a proof\u00a0that:Multiple uses of the card at the same cinema should not be\u00a0linkable.The most popular implementations\u00a0include:", "tags": "", "title": "Attribute Based Credentials"},{"url": "/patterns/Enable-Disable-Functions", "text": "Users frequently have data collected about them, often in situations where it needn't be. Many of these cases are due to good intentioned, expansive, functionality. Not all users seek to take advantage of all functions, however. Some controllers aim to consider this in their\u00a0designs.Not all users desire or benefit from all\u00a0functionality.Consider users living in an Ambient Assisted Living environment: these users are surrounded by various sensors such as video cameras, motion sensors or electrical current sensors that are used to monitor the actual situation of a person. Another example are the acceleration sensors included in smartphones. A [service (or product)] can recommend places of interest to the user by considering the gathered [data]. With regard to these examples it becomes obvious that [services] often unobtrusively collect highly critical and personal context data of\u00a0users.Enable users to choose which functions they do not consent to using, nor wish to provide the required data\u00a0for.A solution is given if the user can explicitly agree or disagree to certain functions. For this purpose, the [service] has to display every function and its required context data. A possible way of displaying these functions and the used context data may be the use of the privacy consent form, which is included in every\u00a0application.By enabling the user to explicitly agree or disagree to certain functions, a context aware application like Support-U might not be able to provide all of its possible functionalities to the user anymore. However, the usage of this pattern in the development process of context-aware applications might additionally strengthen the user's confidence in the usage of UC\u00a0systems.In the shown privacy consent form each function, which utilises personal context information, is listed. Furthermore, the user is able to activate or to deactivate the functions, e.g., to enable a live stream or to enable predicting her next\u00a0context.Meet-U provides several functions that make use of localization mechanisms and the personal data the user supplies. That includes the user's interests, buddy list and [their] preferred means of transportation. For indoor navigation a RFID sensor attached to the user is exploited. The user can now switch off the navigation function so that neither the indoor nor the outdoor localization continue to operate. The user's preferences concerning transportation will be no longer available. Further functions can be disabled correspondingly. Turning off, for example, the advanced search engine would stop using the user's\u00a0interests.Based\u00a0on:", "tags": "", "title": "Enable/Disable Functions"},{"url": "/patterns/Sign-an-Agreement-to-Solve-Lack-of-Trust-on-the-Use-of-Private-Data-Context", "text": "Suggested: Contractual\u00a0ConsentThe controller does not necessarily have the trust of its users, and needs this trust for its services to process their\u00a0data.The service should provide the user with a contractual agreement (featuring privacy policy) which binds the controller to their word, provided that the user consents to the processing of data needed for specific purposes. The agreement should also bind any representative of the controller. It should be straightforward and clear enough for the user to\u00a0comprehend.The service should feature a mechanism (e.g. landing page or unavoidable introduction) prior to collection, which stipulates the need for user consent. There should be a reasonable effort to prevent users from bypassing this\u00a0mechanism.The specific purposes for which their data will be processed should be made clear. The service should, at the same time, outline the contractual obligations it will be held against should the user consent. The user should be able to seek further detail about these obligations without first needing to\u00a0consent.If users decide to consent, they can make this clear by interacting with a mechanism (e.g. button) which clearly represents their agreement to the\u00a0contract.A further implementation could additionally allow the user access to a subset of the service which does not require any data, in order to help justify their consent. This would also alleviate the user's potential apprehension about the time taken to review and inform themselves about their\u00a0decision.The controller, any of their representatives, and their users are tied to the terms of the contract and the legal implications it holds. Any disputes will involve both contract law and privacy\u00a0law.Users may be discouraged to use a service if they are made aware of the risks to their privacy, or introduced to the ways in which their data can be used to reveal\u00a0information.They may also be tempted to consent without reading about the contract or how their data may be used. Therefore it is useful to not force an immediate decision, as this can invalidate the consent as not freely given or\u00a0uninformed.Based\u00a0on:", "tags": "", "title": "Sign an Agreement to Solve Lack of Trust on the Use of Private Data Context"},{"url": "/patterns/Protection-against-tracking", "text": "This pattern avoids the tracking of visitors of websites via cookies.\nIt does this by deleting them at regular intervals or by disabling\ncookies\u00a0completely.This pattern is applicable when personal identifiable information is\ntracked through software tools, protocols or mechanisms such as\ncookies and the\u00a0like.With every single interaction in the web you leave footmarks and clues\nabout yourself. Cookies for example enable webservers to gather\ninformation about web users which therefore affects their privacy and\nanonymity. Web service providers trace user behavior, which can lead\nto user profiling. Also providers can sell the gathered data about\nusers visiting their pages to other\u00a0companies.Restricting usage of cookies on the client side by deleting cookies on\na regular basis e.g. at every start-up of the operating system or\nenabling them case-by-case by deciding if the visited website is\ntrustworthy or not and by accepting a cookie only for the current\nsession. At the highest level of privacy protection cookies are\ndisabled, but as a consequence web services are restricted. Another\nsolution could be that cookies are exchanged between clients, so that\nsophisticated user profiles\u00a0emerge.Restricting a website to not be able to track any of the user's\npersonal identifiable\u00a0informations.With cookies disabled there is no access to sites that require enabled\ncookies for logging in. Other tracking mechanisms for user\nfingerprinting may still work even when cookies are\u00a0disabled.Alice wants to buy shoes and she wants to shop online. She heads to an\nonline shop and searches for shoes but can\u2019t decide which ones she\nwants, so she buys neither of them. The next day she finds a couple of\nemails in her inbox, giving her suggestions for other shoes and\nalerting her that the viewed shoes are now on\u00a0sale.Junkbuster is an old proxy filtering between web server and browser to\nblock ads and cookies, but it is no longer maintained. A program named\nCookieCooker (http://www.cookiecooker.de/) provides protection for\nmonitored user behaviour and interests by exchanging cookies with\nother users or using a random selection of identities. Unfortunately\nthis project also seems to be not maintained anymore. There is also\nthe Firefox Add-on Self-Destructing Cookies which deletes cookies of\ntabs as soon as they are\u00a0closed.", "tags": "", "title": "Protection against Tracking"},{"url": "/patterns/Identity-federation-do-not-track-pattern", "text": "All information has been extracted from\u00a0http://blog.beejones.net/the-identity-federation-do-not-track-patternThe Do Not Track Pattern makes sure that neither the Identity Provider\nnor the Identity Broker can learn the relationship between the user\nand the Service Providers the user\u00a0us.This pattern is focused on identity federation\u00a0modelsWhen an identity system provides identifying information about a user\nand passes this to a third party service, different parties can do\ncorrelation and derive additional\u00a0information.Include an orchestrator component, that must act in behalf and be\ncontrolled by the user. The orchestrator makes sure that the identity\nbroker can\u2019t correlate the original request from the service provider\nwith the assertions that are returned from the identity provider. The\ncorrelation can only be done within the orchestrator but that\u2019s no\nissue because this acts on behalf of the user, possibly on the device\nof the\u00a0user.Avoid the correlation of end user and service provider\u00a0dataIn practice, the orchestrator could run in the browser of the user as\na javascript program or as an App on his\u00a0deviceIdentity federations and\u00a0ecosystems", "tags": "", "title": "Identity Federation Do Not Track Pattern"},{"url": "/patterns/User-data-confinement-pattern", "text": "Avoid the central collection of personal data by shifting some amount\nof the processing of personal data to the user-trusted environments\n(e.g. their own devices). Allow users to control the exact data that\nshares with service\u00a0providersThis pattern may be used whenever the collection of personal data with\none specific and legitimate purpose still pose a relevant level of\nthreat to the users'\u00a0privacyThe engineering process is biased to develop system-centric\narchitectures where the data is collected and processed in single\ncentral entities, forcing users to trust them and share potentially\nsensible personal\u00a0dataThe solution is to shift the trust relationship, meaning that instead\nof having the customer trust the service provide to protect its\npersonal data, the service provider now haves to trust the customers'\u00a0processing.In the smart meter example, the smart meter would receive the monthly\ntariff and calculate the customer's bill which will be then sent to\nthe energy provider where it will be processed. The main benefit is\nthat at no moment the personal data has left the users trusted\u00a0environment.Avoid the need for trust in service providers and the collection of\npersonal\u00a0dataDepending on the type of processing (e.g calculate the bill for the\nmonthly energy consumption or the age from the birth date) the service\nprovider will require some guarantees from the processor (the end\nuser). This may involve the usage of Trusted Platform Modules or\ncryptographic algorithms (e.g.\u00a0ABC4Trust)The smart grid is a domain with a clear example: having smart meters\ndelivering hourly customers' energy consumption to the energy provider\nposes a serious threat to the customers' privacy. If the only purpose\nof collecting these data is to bill the customer, why cannot this\ncalculation be done by the customer based on pre-established\u00a0tariffs?Similar examples in other domains are \"pay as your drive\" insurance\npolicies where the insurance price is calculated based on the drivers\nbehaviour or electronic toll\u00a0pricingSmart meter, Privacy-enhanced attribute based credentials, pay as your\ndrive insurances, electronic toll\u00a0pricing", "tags": "", "title": "User data confinement pattern"},{"url": "/patterns/sticky-policy", "text": "Machine-readable policies are sticked to data to define allowed usage\nand obligations as it travels across multiple parties, enabling users\nto improve control over their personal\u00a0information.Multiple parties are aware of and act according to a certain policy\nwhen privacy-sensitive data is passed along the multiple successive\nparties storing, processing and sharing that\u00a0data.Data may be accessed or handled by multiple parties that share data\nwith an organisation in ways that may not be approved by the data\u00a0subject.Service providers use an obligation management system. Obligation\nmanagement handles information lifecycle management based on\nindividual preferences and organisational policies. The obligation\nmanagement system manipulates data over time, ensuring data\nminimization, deletion and notifications to data\u00a0subjects.The goal of the pattern is to enable users to allow users to control\naccess to their personal\u00a0information.Bene\ufb01ts: Policies can be propagated throughout the cloud to trusted\norganisations, strong enforcement of the policies, traceability.\nLiabilities: Scalability: policies increase size of data. Practicality\nmay not be compatible with existing systems. It may be difficult to\nupdate the policy after sharing of the data and existence of multiple\ncopies of data. It requires ensuring data is handled according to\npolicy e.g. using\u00a0auditing.When data is shared by an organisation they can use privacy preserving\npolicy to enforce respecting user privacy by third party organisations\nthat use, process and store such data. For example, a hospital may\nshare data with third party organisations requiring adhering to\nspecific privacy policies associated with the\u00a0data.", "tags": "", "title": "Sticky Policies"},{"url": "/patterns/Negotiation-of-Privacy-Policy", "text": "Often when users find a service (or product) they would like to use, and begin signing-up, they are immediately exposed to assumptions which may not hold for them. As users have differing privacy priorities, a controller cannot guess as to what settings best accommodate them. Since these preferences may be intricate, users cannot be expected to specify them in detail all at once or before using the\u00a0service.Users have sometimes wildly different priorities regarding their privacy, though a controller does not know these details when a user first joins a service. There is a temptation to provide these users the settings the average user\u00a0uses.As users begin to use a service, determine their individual privacy sensitivities by allowing them to opt-in/opt-out of account details, targeted services, and telemetry. When a user's preference is not known, assume the most privacy-preserving settings. It should always take more effort to over-share than to\u00a0under-share.Unauthenticated users should enjoy the most privacy-preserving defaults. When a user joins the service, they may be presented with [excerpts or summaries of] a privacy policy, which they can use to inform their choices. Using simple, recognizable controls, users can be asked to opt-in (for explained benefits) or opt-out (at explained costs) before any of their data is used. They can then be asked for additional consents further down the line as they become contextually\u00a0relevant.In this way, only the needed consent is asked for as the controller's understanding of the user's preferences improves. This can allow the service to determine which solicitations users are individually likely to consider, and which ones will only waste their time or upset\u00a0them.Private defaults will often not be the appropriate settings for a user, as most users may be less privacy-concerned. The additional effort taken to share more, with users or the controller, will reduce the valuable data collected. However, providing users with invasive defaults would risk public outrage by the vocal few, who may affect opinions\u00a0holistically.Based\u00a0on:J. Porekar, A. Jerman-Bla\u017ei\u010d, and T. Klobu\u010dar, \u201cTowards organizational privacy patterns,\u201d Proceedings - The 2nd International Conference on the Digital Society, ICDS 2008,\u00a02008.", "tags": "", "title": "Negotiation of Privacy Policy"},{"url": "/patterns/Onion-routing", "text": "This pattern provides unlinkability between senders and receivers by\nencapsulating the data in different layers of encryption, limiting the\nknowledge of each node along the delivery\u00a0path.A system in which data is routed between different\u00a0nodes.When delivering data, the receiver has to be known. If the system\nprovides the functionality that the receiver of data should be able to\nanswer, than the receiver should also know the address of the sender.\nWhen forwarding information over multiple stations then, in a naive\nimplementation, each station on the delivery path knows the sender and\nthe final\u00a0destination.The solution is to encrypt the data in layers such that every station\non the way can remove one layer of encryption and thus get to know the\nimmediate next station. This way, every party on the path from the\nsender to the receiver only gets to know the immediate successor and\npredecessor on the delivery\u00a0path.The goal of this pattern is to achieve unlinkability between senders\nand\u00a0receivers.If there are too few hops, the anonymity set is not big enough and the\nunlinkability between sender and receiver is at risk. The same problem\noccurs when there is too few communication going on in the network.\nThe multiple layers of encryption will bloat up the data and consume\nbandwidth. If all nodes on the delivery path collaborate in deducing\nthe sender and the receiver, the pattern becomes\u00a0useless.Alice is a whistle-blower and tries to forward data to Bob who works at\nthe press. She sends the corresponding documents as an\ne-mail-attachment. Eve monitors the traffic and can see who sent this\nmail to whom. The next day, police raids Alice's apartment and sends\nher to jail. Bobs mail account gets\u00a0seized.The TOR-browser, a web-browser specifically designed to ensure\nanonymity makes heavy use of onion\u00a0routing.", "tags": "", "title": "Onion Routing"},{"url": "/patterns/Informed-Consent-for-Web-based-Transactions", "text": "User data is frequently collected for various purposes. Sometimes this data is personal, personally identifying, or otherwise sensitive. The data may serve to improve a service (or product) offered by a controller, or to provide relevant suggestions or advertisements to users. This is particularly prevalent on the web, as many websites derive most of their income from this data. Where income is instead in the form of purchase, user data is nonetheless needed to provide billing or shipping information. This includes auditing, logging, or other non-repudiation purposes to facilitate\u00a0transactions.Before collecting data, controllers must make sure users provide informed\u00a0consent.Controllers need to be able to inform their users about these purposes and means before the user\u00a0consents.Provide the user with clear and concise information regarding what may be learned from their data, and how that data can be used to offer or improve the service. Then acquire their explicit, freely-given\u00a0consent.To the extent possible given the limits imposed by web technology, provide the user with the six elements of informed consent: Disclosure [of purpose specification and limitation,] Agreement [and disagreement capabilities,] Comprehension [through easily understandable, comprehensive\u00a0and concise explanations,] Voluntariness [showing that consent is freely-given,] Competence [to make reasonable legally binding decisions, and] Minimal Distraction [which may otherwise aggravate the\u00a0user].Based\u00a0on:S. Romanosky, A. Acquisti, J. Hong, L. F. Cranor, and B. Friedman, \u201cPrivacy patterns for online interactions,\u201d Proceedings of the 2006 conference on Pattern languages of programs - PLoP \u201906, p. 1,\u00a02006.Fischer-H\u00fcbner, S., K\u00f6ffel, C., Pettersson, J.-S., Wolkerstorfer, P., Graf, C., Holtz, L. E., \u2026 Kellermann, B. (2010). HCI Pattern Collection \u2013 Version\u00a02.C. Bier and E. Krempel, \u201cCommon Privacy Patterns in Video Surveillance and Smart Energy,\u201d in ICCCT-2012, 2012, pp.\u00a0610\u2013615.C. Graf, P. Wolkerstorfer, A. Geven, and M. Tscheligi, \u201cA Pattern Collection for Privacy Enhancing Technology,\u201d The Second International Conferences of Pervasive Patterns and Applications (Patterns 2010), vol. 2, no. 1, pp. 72\u201377,\u00a02010.", "tags": "", "title": "Informed Consent for Web-based Transactions"},{"url": "/patterns/Use-of-dummies", "text": "This pattern hides the actions taken by a user by adding fake actions\nthat are indistinguishable from\u00a0real.This pattern is applicable when it is not possible to avoid executing,\ndelaying or obfuscating the content of an\u00a0action.When users interact with ICT systems their actions reveal a lot of\ninformation about themselves. An option would be for users to not\nperform such actions to protect their privacy. However, this is not\npossible since users cannot completely avoid executing these actions\nbecause they need to perform them to achieve a goal (e.g., search for\na word on the Internet, send an email, search for a\u00a0location).Since the action must be accurately performed, an option to provide\nprivacy is to simultaneously perform other actions in such a way that\nthe adversary cannot distinguish real and fake (often called dummy)\u00a0actions.To hinder the adversary\u2019s ability to infer the user behavior, as well\nas her\u00a0preferences.This pattern entails the need for extra resources to perform the dummy\nactions, both at the side of the user that must repeat the action, and\nat the server side that must process several actions. Sometimes it may\ndegrade the quality of service since the service provider cannot\npersonalize services. It has been demonstrated that generating dummies\nthat are perfectly indistinguishable from real actions (in terms of\ncontent, timing, size, etc...) is very\u00a0difficult.Alice wants to search for an abortion clinic on Google, but she does\nnot want to reveal her intentions of abort to an adversary that may be\neavesdropping this search (e.g., ISP provider, system administrator of\nher workplace,\u00a0etc).The use of this pattern has been proposed to protect privacy in\nlocation based services (the user reveals several locations to the\nservice provider so that her real location is hidden), anonymous\ncommunications (the user sends fake messages to fake recipients to\nhide her profile), web searches (the user searches for fake terms to\nhide her real\u00a0preferences).", "tags": "", "title": "use of dummies"},{"url": "/patterns/Buddy-List", "text": "Contact List, Address\u00a0BookUsers frequently interact upon various media, forums, and communication channels. There are however far more users on these channels than most would be comfortable wading through. As controllers for such channels, many services wish to aid their users in finding familiar and comfortable interactions. Users may also seek to participate outside their immediate circles, but may aim not to stray too\u00a0far.When many users are able to interact in the interaction space, it is hard to maintain an overview of relevant interaction partners since the number of users exceeds the number of relevant contacts for a specific user. User lists grow very large and it is hard to find people who the local user knows. On the other hand, the local user is [more interested in close\u00a0contacts].A service aims to provide users with shortcuts to interaction with users who they are most likely to interact with within a particular context (close contacts within social\u00a0circles).Allow users to find and assign others to a user-maintained directory of social circles and contexts to interact with. This is optionally only visible to the users\u00a0themselves.Users should be able to view the Buddy List on demand, either during a search operation or persistently. They should be able to add or remove users from the relevant list with minimal\u00a0effort.The list may be seen as a set of user objects. This buddy list has the possibility of adding or removing user objects. In the first case, whenever the local users interact with another user, they can add the other user to their buddy list. To reach this goal, in the user interface, the local users can select the representation of the another user and execute a command for adding (e.g. a menu item associated to the user object). For removing users, when the buddy list is shown, the local users can select the representation of the another user and execute a command for removing (e.g. a menu item associated to the user\u00a0object).The Buddy List may fuse with other common interaction idioms to constitute a more comprehensive approach to the problem, making it more than an\u00a0idiom.Connecting the means for adding users to the buddy list with the user\u2019s representation (or the interface elements that are used to interact with the other user) makes the process of adding a user to the buddy list intuitive and reminds a user to consider adding the\u00a0user.By using the Buddy List to make connections about the user, the service can recommend relevant contact\u00a0suggestions.If users only consider buddy lists for maintaining contacts to other users, they will hardly find new users in the system. Thus you should ensure that users can also browse other users who are not on their buddy list (e.g. by providing a User\u00a0Gallery).The service can trivially derive the social structure of its userbase which may put trust at\u00a0jeopardy.Based\u00a0on:T. Sh\u00fcmmer, \u201cThe Public Privacy \u2013 Patterns for Filtering Personal Information in Collaborative Systems,\u201d in Proceedings of CHI workshop on Human-Computer-Human-Interaction Patterns,\u00a02004.", "tags": "", "title": "Buddy List"},{"url": "/patterns/Selective-Access-Control", "text": "Selective Access Control in Forum Software;\nPrivacy Options in Social\u00a0NetworksUsers want to control the visibility of the content being shared, because it may not currently be appropriate for all\u00a0users.Provide users with the option to define the audience of their contributions by specifying the access rules to their\u00a0[content].Implement visual controls to help users to define access control rules when they create or modify\u00a0content.These rules could be defined based on users, groups of users, or based on context-aiding attributes like age or location. For groups, it should be possible to directly define who may view the post being published (e.g. a post with personal data aimed only at a group of close friends). Contextually, it may be possible to define an attribute constraint based on whom in general the post is intended for (e.g. a post aimed at people in a specific town or\u00a0region).Based\u00a0on:Drozd, O. (n.d.). Privacy Patterns Catalog. Retrieved January 25, 2017, from\u00a0http://privacypatterns.wu.ac.at:8080/catalog/Fischer-H\u00fcbner, S., K\u00f6ffel, C., Pettersson, J.-S., Wolkerstorfer, P., Graf, C., Holtz, L. E., \u2026 Kellermann, B. (2010). HCI Pattern Collection \u2013 Version\u00a02.", "tags": "", "title": "Selective access control"},{"url": "/patterns/Support-Selective-Disclosure", "text": "Many services (or products) require the collection of a fixed, often large, amount of personal data before users can use them. Many users, instead, want to freely choose what information they share. This pattern recommends that services Support Selective Disclosure, tailoring functionality to work with the level of data the user feels comfortable\u00a0sharing.Controllers aim to design services to be both maintainable and extensible, though as a result blanket strategies are used to simplify designs. Users are individuals and do not always respond the same way to different approaches. Restricting user choice on processing displeases users, and bundling purposes for that processing conflicts with international law. Users want a service which works without the data they do not want to provide, even so far as effectively anonymous\u00a0usage.Controllers typically want to collect data by default, and tend to limit the diversity of their services, and the choices they provide, to encourage that. This goes against the best interests of the users, who have varying data collection\u00a0tolerances.The underlying issues are discussed in more detail\u00a0below.Controllers are tempted to see consent as all-encompassing, see held personal data as data available for use, and the lack of that data as a barrier to service. This mindset reduces adoption of the offering and may introduce a lack of\u00a0trust.This problem is also present when users register for or acquire a service, as unnecessary information is often requested as part of the process.  In the case of account registration users are often provided with inappropriate default settings. They are typically sent additional offers by default as well. The negative implications of these defaults are also not necessarily reversible, as the Internet is notorious for its inability to\u00a0forget.Services tend to collect a surplus of information, especially in contexts where monitoring is integral to the system, such as in productivity tracking. This unnecessary level of detail results in negative experience factors for the tracked individuals (for e.g. increased levels of anxiety) which in a work environment may affect their actual\u00a0productiveness.Determine what information is integral to the functioning of the system. If functionality may be sustained with less, it should be an option for the user, even if doing so comes with reduced usability. Additionally, provide anonymous functionality only where it cannot jeopardise the service. Lower levels of anonymity may be provided in relation to various capabilities for\u00a0abuse.At one extent it may be possible to benefit from the system anonymously, though whether this is feasible will depend on the level of abuse anonymous usage might attract. Alternatively, this can be approached from the perspective of revocable privacy. That is, tentative or eroding anonymity. If this would result in an unsustainable business model, however, a re-balance of usability may be\u00a0sufficient.It is important to note that while anonymous usage might not translate into direct profit, additional contributors and positive public perception may increase overall user activity. Furthermore, there are payment methods which support [some level of] anonymity if\u00a0necessary.Where users choose to register, it should not be assumed that they wish to use all of the system's services. Short of explicitly opting for 'best experience settings' (with sufficient explanation; not the default option), user preferences should default to the most privacy-friendly\u00a0configuration.User decisions should be amendable. For example, an agreement to share activity with another user may not carry over to all future usage. A user may decide to share something once in a while, or share regularly, but not always. The system should be able to account for this behaviour if it aims to prevent mistaken\u00a0actions.In situations where there are requirements for personal data, particularly when strict, users should be aware of this prior to their consent. These services should also not be coupled with other services holding lower requirements unless it would be infeasible not to. Where users are required to use the system, no unnecessary information should be used. In a productivity tracking example, this may mean that users are only identified when their productivity falls, or perhaps if they opt to receive credit for their\u00a0work.Due to increased control over their data, users may be able to share pieces of information which they otherwise wouldn't due to it otherwise being coupled with what they perceive to be more\u00a0sensitive.Users will be less likely to mistakenly release personal information to the public, since they would perhaps be able to set their own defaults, or by default stay private. To a further extent, users may be capable of participating or benefiting from a system anonymously. Where this is the case, the activity levels of the system will benefit, and users who stayed anonymous due to mixed feelings about the system may decide to register and authenticate later, once trust has been\u00a0built.The system's complexity will increase by a certain degree, as not only will each user need to have their preferences set, stored, and adhered to, but also services will need to account for variable inputs. As such, flaws in the system will be felt with greater\u00a0effect.Providing anonymity for some contexts may result in increased undesired behaviour, depending on the level of anonymity provided. Anonymising a service often requires additional processing power, especially in the case of revocable\u00a0privacy.By separating functionality according to purpose and personal data needed, as well as providing variations where feasible, the system will be more complex. Services will need to be designed while taking into consideration the potential for limited access to\u00a0data.Improvements to results may therefore be limited as well. However, the controller may be able to gauge adoption in data-rich services while they are investing in them. The same holds for determining how valuable non-invasive alternatives are, as users will express their [in]tolerance for invasiveness through their\u00a0actions.S. Ahern, D. Eckles, N. Good, S. King, M. Naaman, and R. Nair, \u201cOver-Exposed ? Privacy Patterns and Considerations in Online and Mobile Photo Sharing,\u201d CHI \u201907, pp. 357\u2013366,\u00a02007.H. Baraki et al., Towards Interdisciplinary Design Patterns for Ubiquitous Computing Applications. Kassel, Germany,\u00a02014.E. S. Chung, J. I. Hong, J. Lin, M. K. Prabaker, J. a. Landay, and A. L. Liu, \u201cDevelopment and Evaluation of Emerging Design Patterns for Ubiquitous Computing,\u201d DIS \u201904 Proceedings of the 5th conference on Designing interactive systems: processes, practices, methods, and techniques, pp. 233\u2013242,\u00a02004.G. Iachello and J. Hong, \u201cEnd-User Privacy in Human-Computer Interaction,\u201d Foundations and Trends in Human-Computer Interaction, vol. 1, no. 1, pp. 1\u2013137,\u00a02007.J. Porekar, A. Jerman-Bla\u017ei\u010d, and T. Klobu\u010dar, \u201cTowards organizational privacy patterns,\u201d Proceedings - The 2nd International Conference on the Digital Society, ICDS 2008,\u00a02008.S. Romanosky, A. Acquisti, J. Hong, L. F. Cranor, and B. Friedman, \u201cPrivacy patterns for online interactions,\u201d Proceedings of the 2006 conference on Pattern languages of programs - PLoP \u201906, p. 1,\u00a02006.T. Sch\u00fcmmer, and J. M. Haake (2001). \u201cSupporting distributed software development by modes of collaboration,\u201d in Proceedings of ECSCW 2001,\u00a0Bonn.T. Sh\u00fcmmer, \u201cThe Public Privacy \u2013 Patterns for Filtering Personal Information in Collaborative Systems,\u201d in Proceedings of CHI workshop on Human-Computer-Human-Interaction Patterns,\u00a02004.", "tags": "", "title": "[Support] Selective Disclosure"},{"url": "/patterns/Discouraging-blanket-strategies", "text": "Socially oriented services on the Internet allow their often diverse userbase to share content. These masses of users and shared content are also varied enough to discourage individual attention. Controllers prefer to protect themselves from additional complexity and investment into features which provide them with less data. Users, however, feel in need of privacy settings to distinguish their personal risk appetite from that of the norm. They each have their own ideas about the sensitivities of their information, which makes sufficient controls difficult to\u00a0implement.Overly simplified privacy settings following all or nothing strategies could result in over-exposure, self-censoring, and unsatisfied\u00a0users.These all or nothing strategies could refer to privacy settings which holistically apply to all content, or to binary (or otherwise deficient) choices for public\u00a0visibility.Provide users with the possibility to define a privacy level for content being shared with the controller, or with other users. Give them a range of visibilities, so that they can decide the access-level of the content being shared according to different users, or service-defined\u00a0groups.Provide users easily-recognizable visual elements to define the privacy level for each content submission. Use controls, such as (drop-down) lists, combo boxes, etc. to provide a range of possible privacy\u00a0levels.The privacy controls themselves also need to be designed in such a way that it is very clear to users what each setting does and what it means for their\u00a0privacy.Based\u00a0on:S. Ahern, D. Eckles, N. Good, S. King, M. Naaman, and R. Nair, \u201cOver-Exposed ? Privacy Patterns and Considerations in Online and Mobile Photo Sharing,\u201d CHI \u201907, pp. 357\u2013366,\u00a02007.", "tags": "", "title": "Discouraging blanket strategies"},{"url": "/patterns/identity-federation-do-not-track-pattern", "text": "All information has been extracted from\u00a0http://blog.beejones.net/the-identity-federation-do-not-track-patternThe Do Not Track Pattern makes sure that neither the Identity Provider\nnor the Identity Broker can learn the relationship between the user\nand the Service Providers the user\u00a0us.This pattern is focused on identity federation\u00a0modelsWhen an identity system provides identifying information about a user\nand passes this to a third party service, different parties can do\ncorrelation and derive additional\u00a0information.Include an orchestrator component, that must act in behalf and be\ncontrolled by the user. The orchestrator makes sure that the identity\nbroker can\u2019t correlate the original request from the service provider\nwith the assertions that are returned from the identity provider. The\ncorrelation can only be done within the orchestrator but that\u2019s no\nissue because this acts on behalf of the user, possibly on the device\nof the\u00a0user.Avoid the correlation of end user and service provider\u00a0dataIn practice, the orchestrator could run in the browser of the user as\na javascript program or as an App on his\u00a0deviceIdentity federations and\u00a0ecosystems", "tags": "", "title": "Identity Federation Do Not Track Pattern"},{"url": "/patterns/Encryption-user-managed-keys", "text": "Use encryption in such a way that the service provider cannot decrypt the user's information because the user manages the\u00a0keys.Enable encryption, with user-managed encryption keys, to protect the confidentiality of personal information that may be transferred or stored by an untrusted 3rd\u00a0party.User wants to store or transfer their personal data through an online service and they want to protect their privacy, and specifically the confidentiality of their personal information. Risks of unauthorized access may include the online service provider itself, or third parties such as its partners for example for backup, or government surveillance depending on the geographies the data is stored in or transferred\u00a0through. How can a user store or transfer their personal information through an online service while ensuring their privacy and specifically preventing unauthorized access to their personal\u00a0information?Requiring the user to do encryption key management may annoy or confuse them and they may revert to either no encryption, or encryption with the online service provider managing the encryption key (affording no protection from the specific online service provider managing the key), picking an encryption key that is weak, reused, written down and so\u00a0forth. Some metadata may need to remain unencrypted to support the online service provider or 3rd party functions, for example file names for cloud storage, or routing information for transfer applications, exposing the metadata to risks of unauthorized access, server side indexing for searching, or\u00a0de-duplication. If the service provider has written the client side software that does the client side encryption with a user-managed encryption key, there can be additional concerns regarding whether the client software is secure or tampered with in ways that can compromise\u00a0privacy.Encryption of the personal information of the user prior to storing it with, or transferring it through an online service. In this solution the user shall generate a strong encryption key and manage it themselves, specifically keeping it private and unknown to the untrusted online service or 3rd\u00a0parties.", "tags": "", "title": "Encryption with user-managed keys"},{"url": "/patterns/Pay-Back", "text": "In services where users may contribute content, or provide the system with account or profile information, the information is only valuable if relevant and accurate. For controllers providing this service (or product), worthless information does not typically generate income or future participation. Without consistent usage, a service becomes less popular and eventually may run at loss. This is particularly true in socially oriented services. To keep the service working, it is crucial that its users maintain content. Users however, might not feel inclined to do so. Keeping content up to date, or adding it in the first place, requires effort, and in some cases an acceptance of privacy\u00a0risk.Users do not necessarily want to provide and maintain content, they need a motivation to do so. Without this, a service will not\u00a0flourish.Provide users with different kinds of benefits when they contribute or maintain content for the service and make sure they do so\u00a0consensually.Depending on the kind of service that is provided, different benefits could be considered: virtual or real currency, use of services, social benefits, and so\u00a0on.When using virtual or real currency, the controller should first define how much in value users would receive depending on the contributions. In the case of virtual currency, the places where the currency could be used should be\u00a0defined.Regarding use of service, some criteria could be considered non-exhaustively: feedback on content, frequency of contributions, the use of service for a minimum duration, access to a service earlier than others, or use of special features within the\u00a0service.When users reach a limit, they could additionally assist with virtual or physical events for learning, meeting people, etc. In virtual scenarios, users could receive attention (feedback) from one\u00a0another.Based\u00a0on:T. Sch\u00fcmmer, \u201cThe Public Privacy \u2013 Patterns for Filtering Personal Information in Collaborative Systems,\u201d in Proceedings of CHI workshop on Human-Computer-Human-Interaction Patterns,\u00a02004.", "tags": "", "title": "Pay Back"},{"url": "/patterns/Trustworthy-privacy-plugin", "text": "Aggregate usage records at the user side in a trustworthy\u00a0manner.A service provider gets continuous measurements of a service attribute linked to a service individual. Applicable service tariffs may vary over\u00a0time.The provision of a service may require repeated, detailed measurements of a service attribute linked to a data subject to e.g. properly bill them for the service usage. However, these measurements may reveal further information (e.g. personal habits, etc.) when repeated over\u00a0time.Host a Privacy Plugin at a consumer-trusted device, in between the metering and the billing systems. and the service provider in charge of billing for the service usage. This privacy plugin, under the consumer\u2019s control, computes the aggregated invoice and sends it to the service provider (or to its billing subsystem), which does not need any fine-grained consumption records anymore. Cryptographic techniques (homomorphic commitments, zero-knowledge proofs of knowledge, digital signatures) are used to ensure trustworthiness of the generated invoices without requiring tamper-proof\u00a0hardware.A service provider can get a trustworthy measurement of service usage along a period to issue a bill for the service usage; however, the detailed consumption for finer intervals cannot be\u00a0obtained.The service provider does not need anymore to access detailed consumption data in order to issue reliable\u00a0bills.An electric utility operates a smart grid network with smart meters that provide measurements of the instantaneous power consumption of each user. Depending on the power demand, dynamic tariffs are applied. The utility employs that information to bill each client periodically, according to his aggregated consumption over the billing period and the respective tariffs at each moment. However, this information can also be exploited to infer sensitive user information (e.g. at what time he or she leaves and comes back to home,\u00a0etc.)", "tags": "", "title": "Trustworthy Privacy Plug-in"},{"url": "/patterns/Reciprocity", "text": "Fair distribution of efforts / Win-win\u00a0situationIn services where users may either socially or collaboratively contribute, participation may be a foundation for the service's business model. In these situations the quality and frequency of content affects the success of the service, and thus users have a large impact on its survival. Whether any single user contributes, or not, plays a role in profitability, which puts the controller in a position to encourage or enforce equal participation. Users may respond to such ideas negatively, however, especially if they do not see potential gains worthy of their effort and personal risks to\u00a0privacy.Equal participation does not always result in equal rewards. In some cases, participants do not need to contribute at all to benefit from the content generated by the group. Any who feel slighted are then likely to contribute less, eventually jeopardizing results for the\u00a0group.Limit the benefits gained from the group effort to the amount of effort contributed. All contribution should be afforded proportionate\u00a0gains.Ensure that all group members' activities result in an improved group result that is beneficial for all group members again. Prohibit people to benefit from group results if they are not willing to help the group in\u00a0return.Prior to completing designs on functionality, determine the benefits as opposed to efforts or costs on all possible user activities. Weigh these, with input from any necessary stakeholders. Any feature which does not affect more than one user does not need to be\u00a0assessed.For user groups that are able to affect one another within a feature or functionality, consider them each a case for a collaboration mode. If a user within this group performs an activity, they are expected to reciprocate on any benefits (or gain from costs). This is in proportion to the weighted effort of the feature determined\u00a0earlier.The way in which users reciprocate is up to specific implementation. It may include required effort (satisfied by certain activities) before their activity's resulting benefit is realized. Alternatively it may prevent additional beneficial activities until they contribute. It may also make their discrepancy public, allowing the users to determine tolerable thresholds. In all these cases it is useful to keep track of each user's ratio within each collaboration mode they\u00a0feature.It is important that any use of user data is done so under the explicit and properly obtained permissions required. Deriving value from participation rewards users for providing personal information, and thus they must be informed about how their data may be\u00a0used.As per Sch\u00fcmmer (2004), it is also complemented by the following patterns which are invasive by design, but introduce useful notions:\n- Show the Expert (dark pattern), showing contributions prominently;\n- Who\u2019s Listening (dark pattern), feedback by\u00a0subscribers.Based\u00a0on:T. Sch\u00fcmmer, \u201cThe Public Privacy \u2013 Patterns for Filtering Personal Information in Collaborative Systems,\u201d in Proceedings of CHI workshop on Human-Computer-Human-Interaction Patterns,\u00a02004.", "tags": "", "title": "Reciprocity"},{"url": "/patterns/Layered-policy-design", "text": "Split privacy policies into nested, successively refined versions. Leave the legalese to the\u00a0lawyers.Split privacy policies into nested, successively refined\n    versions. Leave the legalese to the\u00a0lawyers.A data controller offers detailed, legal explanations of their privacy and data protection\u00a0policies.Privacy policies may be difficult to understand and hard to read. What was initially conceived as an instrument to inform users is now almost useless, as they have become riddled with legalese and all sort of extraneous details. As a consequence, users do not read the privacy policies, for being long and\u00a0cumbersome.However, privacy policies are legally binding documents, which makes it difficult to get just rid of these legal\u00a0aspects.A short notice may provide a summary of the practices that deal with personal data, highlighting those which may not be evident to the data subject. Then, a longer policy may provide specific information, split into sections, detailing any uses of personal data. And finally, the whole legal text of the privacy policy can be\u00a0specified.Make users really understand what they can expect about their personal data from a data controller (in terms of which data is managed, for which purposes,\u00a0etc.)The use of this pattern fosters simplicity, transparency and\u00a0choice.However, two versions of the privacy policies coexist, which may introduce potential contradictions; in particular, the data controller must ensure that updates are performed in parallel and\u00a0coherently.See examples at Terms of Service Didn't Read. The average user would take 76 work days to read the privacy policies they encounter each\u00a0year", "tags": "", "title": "Layered Policy Design"}]}