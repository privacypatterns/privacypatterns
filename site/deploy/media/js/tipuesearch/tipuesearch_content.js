var tipuesearch = { "pages": [{"url": "/patterns/Unusual-activities", "text": "Use additional factors to notify users of unusual activities and authenticate \nwhen accounts may have been\u00a0compromised.For Internet services, prevent suspicious access to the account, and/or \nmake the account owner aware of unusual\u00a0activities.Many Internet services are using password-based authentication which is\nreally convenient (compared to strong authentication) but has apparent\u00a0drawbacks:The password itself does not\u00a0change.Unless the account owner changes the password, all sign-ins use the\nsame identity-password combination. If the password is stolen by a\nmalicious party through some means, it can be reused until it is\u00a0changed.The password can be\u00a0cracked.Many account owners tend to use weak passwords that can be\nbrute-force attacked online. Sometimes the hacker can obtain the\ndatabase dump which contains hashed passwords, which can be\nbrute-forced\u00a0offline.The account owner has no way to ensure their exclusive possession of\n    the\u00a0password.The account owner has no evidence if the password is stolen by a\nmalicious\u00a0party.The only way to control who can access is to reset the\u00a0password.As a result, if the account owner want to ensure only they have the\npassword, they can only reset the password. In some scenarios,\nresetting the password is not\u00a0enough.The password can be seen by people\u00a0around.When the account owner enters the password on a computer keyboard or\nthe screen of a mobile device (mobile phone / tablet), people around\nhave the ability to snoop the password and record\u00a0it.Thus, a service can't tell if the user is legitimate even if the\nidentity-password combination provided by the user is correct.\nFortunately, a service can usually receive some meta information to help\ndetermine if an activity is unusual. In case of such unusual activities,\na service needs to prevent suspicious access to an account, and/or\ninform the account owner of the unusual\u00a0activities.How can a service that uses username-password authentication and\ninvolves a lot of privacy identify unusual sign-ins, confirm the\nidentify of the user, and inform the account owner of such unusual\u00a0activities?The pattern described here is a tradeoff between pure password\nauthentication which is insecure, and pure multi-factor authentication\nwhich is inconvenient. It increases privacy at the cost of\u00a0usability.The fallback multi-factor authentication should be picked carefully with\nthe understanding of the\u00a0service.The strategy to identify unusual activities should also be considered\nseriously. It should be also to identify most of the activities, with a\nfalse positive rate that is not too high. It should balance the cost of\nmulti-factor\u00a0authentication.First, the service should be able to identify unusual sign-ins. Then the\nservice may use multi-factor authentication to confirm the identity of\nthe\u00a0user.The user should be informed of unusual activities, or have some ways to\nsee recent events, and even do\u00a0something.Today, a web service may appear as a website or an application on the\nuser's devices (including mobile devices and the PCs). The service can\nuse meta information to determine if a sign-in with the correct\nusername-password combination is\u00a0suspicious.The strategies described here has both false positives and false\u00a0negatives.By running native code, the application can collect some identifiers of\nthe machine, including the operating system environment settings (e.g.\nthe list of running processes), the hardware parameters (such as the ID\nof the CPU), and device UUIDs (provided by mobile operating systems like\niOS). By completing a network request, the service also retrieves the IP\naddress of the\u00a0machine.In case of a suspicious sign-in, multi-factor authentication may be a\nway to let the legitimate user in. The service can request one more\nauthentication except password, such\u00a0as:A software\u00a0tokenExamples include Google Authenticator which runs on mobile phones\nand implements RFC6238 TOTP security\u00a0tokens..A hardware token\u00a0(disconnected)Examples include a token issued by a bank which displays digits,\nwhich is similar to a software\u00a0token.A hardware token\u00a0(connected)The token may exchange a longer secondary password than the previous\none, which means it's\u00a0safer.Personal data like date of birth,\u00a0SSNObviously not a good choice here because it cannot be\u00a0changed.An one-time password (OTP) sent to the registered E-mail address /\n    mobile\u00a0phoneDepending the type of the service, maybe the user uses the same\npassword for the E-mail address, or maybe the mobile phone is stolen\nand the service runs on the mobile\u00a0phone.Using multi-factor authentication only in case of suspicious sign-ins is\nmore convenient to using it all the time, but is less\u00a0secure.When an suspicious sign-in is detected, it may be a sign that the\npassword has already been leaked. Depending on the type of the service,\nit can notify the user about the suspicious sign-in through E-mail,\ntelephone, or other\u00a0means.Here the immediate notification can also be used in the multi-factor\u00a0authentication.For services that can be logged on from multiple devices at the same\ntime, the user should be able to check the existence of other sessions,\nand review recent sign-in\u00a0events.This pattern has some limitations. For example, it relies on accurate\nidentification of suspicious sign-ins based on meta information, where\nthe meta information including the IP address can be spoofed by an\nexperienced\u00a0attacker.If the fallback multi-factor authentication only happens occasionally to\nthe legitimate account owner, they may be unprepared to such\nauthentication, leading to a decreased\u00a0usability.Gmail displays information about other sessions (if any) in the footer,\n linking to a page named \"Activity on this account\" which lists other\n sessions and recent activities to the Gmail account. The user has the\n option to sign out other\u00a0sessions.In case of annoying false positives, the user may choose to disable the\n alert for unusual activity. The disable takes about a week, \"to make\n sure the bad guys aren't the ones who turned off your\u00a0alerts.\"It also displays all devices that are linked to the account, and allows\n the user to unlink one or more of\u00a0them.I started with Gmail's display of account activities. It displays\nunusual activities regarding an account, which involves identifying\nunusual activities where the password entered is correct. For some other\nservices, correct passwords can be rejected from a new device /\u00a0location.So, the scope of this pattern is to handle unusual activities (including\u00a0sign-ins).This pattern includes multi-factor authentication and two-step\nauthentication, which are well studied. But the general topic about\ninforming the user of unusual activities seems to be lack of\u00a0literature.", "tags": "", "title": "Handling unusual account activities with multiple factors"},{"url": "/patterns/Added-noise-measurement-obfuscation", "text": "Add some noise to service operation measurements, but make it cancel itself in the\u00a0long-termA service provider gets continuous measurements of a service attribute linked to a service\u00a0individual.The provision of a service may require repeated, detailed measurements of a service attribute linked to a data subject to e.g. properly bill them for the service usage, or adapt the service according to the demand load. However, these measurements may reveal further information (e.g. personal habits, etc.) when repeated over\u00a0time.A noise value is added to the true, measured value before it is transmitted to the service provider, so as to obfuscate it. The noise abides by a previously known distribution, so that the best estimation for the result of adding several measurements can be computed, while an adversary would not be able to infer the real value of any individual measurement. Note that the noise needs not be either additive or Gaussian. In fact, these may not be useful for privacy-oriented obfuscation. Scaling noise and additive Laplacian noise have proved more useful for privacy\u00a0preservation.A service provider can get reliable measurements of service attributes to fulfil its operating requirements; however, no additional personal information can be inferred from the aggregation of several measurements coming from the same\u00a0user.The pattern applies to any scenario where the use of a resource over time is being monitored (e.g. smart grid, cloud computing). The device providing the measurement must be trustworthy, in order to ensure that it abides by the established noise\u00a0pattern.Some information is lost due to the noise added. This loss of information may prevent the information from being exploited for other purposes. This is partly an intended consequence (e.g. avoid discovering user habits), but it may also preclude other legitimate\u00a0uses.In order for information to be useful after noise addition, the number of data points over which measurements are aggregated (i.e. the size of the aggregated user base) needs to be high; otherwise, either the confidence interval would be too broad or differential privacy could not be effectively\u00a0achieved.The pattern applies to any scenario where the use of a resource over time is being monitored (e.g. smart grid, cloud computing). The device providing the measurement must be trustworthy, in order to ensure that it abides by the established noise\u00a0pattern.Some information is lost due to the noise added. This loss of information may prevent the information from being exploited for other purposes. This is partly an intended consequence (e.g. avoid discovering user habits), but it may also preclude other legitimate\u00a0uses.In order for information to be useful after noise addition, the number of data points over which measurements are aggregated (i.e. the size of the aggregated user base) needs to be high; otherwise, either the confidence interval would be too broad or differential privacy could not be effectively\u00a0achieved.An electric utility operates a smart grid network with smart meters that provide measurements of the instantaneous power consumption of each user. The utility employs that information to both adapt the power distribution in a dynamic fashion, according to user demand at each moment, and bill the each client periodically, according to his aggregated consumption over the billing period. However, this information can also be exploited to infer sensitive user information (e.g. at what time he or she leaves and comes back to home,\u00a0etc.)", "tags": "", "title": "Added-noise measurement obfuscation"},{"url": "/patterns/Private-link", "text": "Enable sharing and re-sharing without wide public visibility or cumbersome authenticated access\u00a0control.When private content is accessible online, especially granular resources, and users want to share (and enable re-sharing) using existing communication mechanisms. Also particularly relevant when users are sharing with contacts who can't easily\u00a0authenticate.How do you share a private resource with unauthenticated users in a way that respects its sensitivity?\nThe solution must not allow unauthenticated users to access resources that weren't intended to be\u00a0shared. Be sure the link is unguessable (within some bounds) so that an outsider can't easily find all (or any) of the unlisted links either by brute force or by knowing when information was stored or what its name might\u00a0be.Note that the URL will be retained in recipients' browser history and could easily be inadvertently shared with others. Services should help users understand these\u00a0limitations.Services may also allow users to revoke existing private links or change the URL to effectively re-set who can access the\u00a0resource.Flickr \"Guest\u00a0Pass\"Google \"anyone with the link\"\u00a0sharingTripit \"Get a\u00a0link\"Dropbox \"Share\u00a0Link\"", "tags": "", "title": "Private link"},{"url": "/patterns/Privacy-aware-network-client", "text": "A privacy policy which is hard to understand is in an automated way\nconverted into a more easy to read\u00a0format.This pattern is limited to the web browsing\u00a0domain.Many websites have privacy policies which are hard to understand for\nthe general\u00a0audience.Many people enter websites with different intentions like shopping,\nresearch, etc. At the same time those responsible collect, use and\nrelease information about a user by explaining it through statements\ncalled privacy policies. These privacy policies are not easy to read\nand to\u00a0understand.The problem can be solved by designing and implementing a privacy\nproxy that can parse and interpret policies. Afterwards it translates\nthe policies into a human-readable format to present them in a\nuser-friendly\u00a0way.Expand the awareness of the user towards privacy policies of a\u00a0website.The user's awareness of the privacy policy rises so that more informed\ndecisions can be made. The proxy is able to automatically detect\nchanges of the privacy policy. A separate secure connection is needed\nfor the proxy for every access to an area which is secured by a\nprivacy policy. Policy constraints need to allocate local storage\nin the client. An attack on this could lead the user to decisions\nwhich he would otherwise not do. If there are any breaches of\nprivacy it can be blamed on the client if he did use a\nprivacy-aware client for a particular\u00a0access.Alice uses several web services but is not aware of the their privacy\npolicies. Even when she reads the policies, she is still not aware of\nthe actual implications of the legal description. In the absence of\nother solutions, she does not read the policies and does not\nunderstand the\u00a0ramifications.JRC P3P Proxy Version 2.0 is a P3P user agent acting like an\nintermediary. Depending on the specified privacy preferences of a\nuser, it controls the access to web servers. Another known P3P user\nagent is AT&T Privacy Bird. Privacy Bird is a tool warning users if\nprivacy policies of visited websites are not matching with their\nindividual privacy\u00a0preferences.", "tags": "", "title": "Privacy-Aware Network Client"},{"url": "/patterns/Ambient-notice", "text": "Provide an ambient notice (unobtrusive, non-modal) when location is being accessed to increase awareness of ongoing\u00a0tracking.Provide unobtrusive, ongoing notice of real-time location\u00a0tracking.How can a service effectively provide notice to a user that location tracking is continuing, or happening again, without being as obtrusive as a repeated modal dialog? A user may not realize that an application given permission to access location is doing so continuously or repeatedly, or may not remember that explicit permissions given in the past allow a service to access data again later. In some cases, past explicit permission may not have been provided by the current user of the device (but instead by a spouse, parent or even an ex-spouse or stalker who temporarily had control of the device or the account). If notice is provided only at the time of consent, a user may inadvertently distribute personal information over a long period of time after having lost control of their device only\u00a0momentarily.A tray full of ambient notices may annoy or confuse users and inure them to ongoing practices. Take measures to avoid unnecessary notice. This must be balanced against the\nconcerns of an attacker's opting the user in without their\u00a0knowledge.Mac OS X Lion adds an ambient location services icon (a compass arrow) which appears in the task bar momentarily when an application is accessing the device's\u00a0location.Chrome adds a cross-hair icon to the location bar when a web site accesses the device location via the W3C Geolocation API. Clicking on the icon provides potential actions: clearing the saved consent for this site and accessing\u00a0settings.Similar examples exist in at least Android, iOS and\u00a0Win7.", "tags": "", "title": "Ambient notice"},{"url": "/patterns/Pseudonymous-messaging", "text": "A messaging service is enhanced by using a trusted third party to\nexchange the identifiers of the communication partners by\u00a0pseudonyms.This pattern can be used for online communications by email, through\nmessage boards, and\u00a0newsgroups.Messaging includes all forms of communication through emails,\narticles, message boards, newsgroups etc. This information could be\nstored and used to build sophisticated user profiles. Sometimes it can\nalso be used to prosecute\u00a0people.A message is send by a user to the server, which exchanges the\nsender's address with a pseudonym. Replied messages are sent back to\nthe pseudonymous address, which will then be swapped back to the\u00a0original.The goal of this pattern is to prevent unforeseen ramifications of the\nuse of online messaging\u00a0services.Users can communicate more freely. Pseudonym servers can be misused to\nsend offensive messages, for spam mails or by criminals for illegal\nactivities. Under those circumstances it could be necessary to revoke\nthe pseudonymity of the corresponding\u00a0parties.Alice is a political activist and tries to organize a political\ndemonstration. Since her government does not like free speech, her\ncommunication channels are intensely monitored and one day, she simply\ndisappears into a labor camp and is never seen\u00a0again.nym.alias.net a pseudonymous email system with the goal to provide\nsecure concealment of the user's identity. A Type I Anonymous Remailer\nforwards emails by modifying the message header and removing\nsender-related\u00a0information.", "tags": "", "title": "Pseudonymous Messaging"},{"url": "/patterns/Obligation-management", "text": "The pattern allows obligations relating to data sharing, storing and\nprocessing to be transferred and managed when the data is shared\nbetween multiple\u00a0parties.The developer aims to make sure that multiple parties are aware of and\ncomply with required user/organisational policies as personal and\nsensitive data are successively shared between a series of parties who\nstore or process that\u00a0data.Data may be accessed or handled by multiple parties that share data\nwith an organisation in ways that may not be approved by the data\u00a0subject.Service providers use an obligation management system. Obligation\nmanagement handles information lifecycle management based on\nindividual preferences and organisational policies. The obligation\nmanagement system manipulates data over time, ensuring data\nminimization, deletion and notifications to data\u00a0subjects.Benefits: privacy preferences and policies are communicated and adhered\nto among organisations sharing data. Liabilities: additional effort to\nset\u00a0obligations.A service provider subcontracts services, but requires that the data\nto be deleted after a certain time and that the service provider\nrequires to be notified if there is further\u00a0subcontracting.Pretschner et al (2009) provide a framework for evaluating whether a\nsupplier is meeting customer data protection obligations in\ndistributed systems. Researchers at IBM propose Enterprise Privacy\nAuthorization Language (EPAL) (2004) to govern data handling practices\naccording to fine-grained access control. Casassa Mont (2004) discusses\nvarious important aspects and technical approaches to deal with\nprivacy obligations. Pretschner, A., Schtz, F., Schaefer, C., and\nWalter, T.: Policy Evolution in Distributed Usage Control. Electron.\nNotes Theor. Comput. Sci. 244, 2009 IBM, The Enterprise Privacy\nAuthorization Language (EPAL), EPAL specification,\nhttp://www.zurich.ibm.com/security/enterprise-privacy/epal/, 2004\nMont, M. C., Dealing with Privacy Obligations, Important Aspects and\nTechnical Approaches, TrustBus,\u00a02004", "tags": "", "title": "Obligation Management"},{"url": "/patterns/attribute-based-credentials", "text": "Attribute Based Credentials (ABC) are a form of authentication mechanism that allows to flexibly and selectively authenticate different attributes about an entity without revealing additional information about the entity (zero-knowledge\u00a0property).ABC can be used in a variety of systems including Internet and smart\u00a0cards.Authentication of attributes classically requires full and unique authentication of an entity. For example, attributes (like age) could be put into a certificate together with name of the user, email address, public key, and other data about that entity. To corroborate an attribute (for example, that the user is an adult) the certificate has to be presented and all information have to be revealed. This is not considered a privacy-preserving\u00a0solution.There are multiple schemes to realize ABCs and implementations are also available. They typically all include a managing entity that entitles issuers to issue credentials to entities that could then act as provers of certain facts about the credentials towards\u00a0verifiers.A formal model can be found\u00a0here.To allow a user to selectively prove specific attributes like age > 18 to a verifying party without revealing any additional\u00a0information.ABC schemes require substantial compute power or optimization, so implementation may not be straightforward. Some projects like IRMA developed at Radboud University Nijmegen have shown that even resource restricted devices like smartcards can implement\u00a0ABCs.You want to issue an ID card that holds a users birthdate bd and can be used to prove that the card holder is old enough to view age-restricted movies in a cinema. Depending on the rating of the movie (minimum age x), the card holder can run a proof\u00a0that:Multiple uses of the card at the same cinema should not be\u00a0linkable.The most popular implementations\u00a0include:", "tags": "", "title": "Attribute Based Credentials"},{"url": "/patterns/Personal-data-store", "text": "Subjects keep control on their personal data that are stored on a\npersonal\u00a0device.The pattern is applicable to any data produced by the data subject (or\noriginally under his control) as opposed to data about him produced by\nthird\u00a0parties.Data subjects actually lose control over their data when they are\nstored on a server operated by a third\u00a0party.A solution consists in combining a central server and secure personal\ntokens. Personal tokens, which can take the form of USB keys, embed a\ndatabase system, a local web server and a certificate for their\nauthentication by the central server. Data subjects can decide on the\nstatus of their data and, depending on their level of sensitivity,\nchoose to record them exclusively on their personal token or to have\nthem replicated on the central server. Replication on the central\nserver is useful to enhance sustainability and to allow designated\nthird parties (e.g. health professionals) to get access to the\u00a0data.Enhance the control of the subjects on their personal\u00a0data.Data subjects need to be equipped with a personal data\u00a0store.Patients want to keep control over their health data but also to grant\nspecific access to some health\u00a0professionals.It has even been deployed for certain types of services, in\nparticular, in the health\u00a0sector.", "tags": "", "title": "Personal Data Store"},{"url": "/patterns/Federated-privacy-impact-assessment", "text": "The impact of personal information in a federation is more than the\nimpact in the\u00a0federatedIdentity Management scenarios (that is, when the roles of the Identity\nProvider and the Service Provider are\u00a0separated).Identity Management solutions were introduced to decouple the\nfunctions related to authentication, authorization, and management of\nuser attributes, on the one hand, and service provision on the other\nhand. Federated Identity Management allows storing a data subject's\nidentity across different systems. All together, these form a\nFederation that involves complex data\u00a0flows.Federated Management solutions can be used to improve privacy (e.g. by\nallowing service providers to offer their services without knowing the\nidentity of their users). However, the complexity of data flows and\nthe possibility of collusion between different parties entail new\nrisks and threats regarding personal\u00a0data.A Privacy Impact Assessment is conducted by all the members of the\nfederation, both individually and in conjunction, so as to define\nshared privacy policies, prove they are met, and demonstrate the\nsuitability of the architecture, in the benefit of all the\u00a0members.Deal with privacy risks associated from the federation of different\nparties in an Identity Management\u00a0solution.The consequences depend on the results of the privacy-impact\u00a0analysis.An Identity Provider issues pseudonyms to authenticate users at\nthird-party Service Providers, which can in turn check the\nauthenticity of these pseudonyms at the Identity Provider, without\ngetting to know the real user identity. However, the Identity Provider\nknows all the services requested by the users, which discloses\npersonal information to the Identity Provider and allows it to profile\nthe\u00a0users.", "tags": "", "title": "Federated Privacy Impact Assessment"},{"url": "/patterns/Data-breach-notification-pattern", "text": "This pattern assures that a certain minimum data breach notification delay is not\u00a0exceeded.This pattern is applicable in any environment where PII is stored and that allows monitoring of specific\u00a0events.In case a data breach has occurred, i.e. Personally Identifiable Information (PII) has leaked, the data owner must be notified. The notification process in turn may not work correctly, so it has to be\u00a0monitored.A monitoring system logs access to clients' PII along with a time-stamp. A notification process continuously verifies that only authorized access is listed in this log file, and in case of unauthorized access notifies the data owner and logs the notification action in the log file, again accompanied by a time-stamp. A notification monitoring process finally continuously checks that t_n - t_l <= max_np (t_n denoting the time of notification, t_l the time of data leakage, max_np the maximally allowed period of notification). In case t_n - t_l > max_np it alerts the PII Incident\u00a0Manager.The pattern goal is to constantly ensure a minimum delay of notification, should a data breach have occurred, and in case a notification exceeds the allowed delay, to indicate this by appropriate\u00a0means.In order to use the pattern, the personal data processor must have in place an access control mechanism and a monitoring mechanism that allows to monitor every PII access. The pattern cannot ensure that a PII Incident Manager will take adequate actions, hence this process has to be established and controlled by other\u00a0means.Assume a company Comp stores all employees' data with a storage service Store. There is a contractual agreement between Comp and Store that each data leakage is reported within one hour. Now Bob, an employee of Store and not authorized to read Comp's data, succeeds in circumventing Store's access control mechanisms and reads Store's data. This represents a data breach of which Comp has to be notified within an\u00a0hour.This pattern is based on the privacy principle \"Accountability\" specified in ISO/IEC 29100 that is also used in Annex A of ISO/IEC 27018. More specifically, it addresses A.9.1 Notification of a data breach involving Personally Identifiable Information (PII). Uses of the pattern as a concrete instantiation of A.9.1 are not\u00a0known.", "tags": "", "title": "Data Breach Notification Pattern"},{"url": "/patterns/TEST-patterns-stub", "text": "This is a test pattern - used for rendering tests. Please\u00a0ignorelorem\u00a0ipsum", "tags": "", "title": "THIS IS A TEST PATTERN - STUB"},{"url": "/patterns/Location-granularity", "text": "Support minimization of data collection and distribution. Important when a service is collecting location data from or about a user, or transmitting location data about a user to a\u00a0third-party.When a service is collecting location data from or about a user, or transmitting location data about a user to a\u00a0third-party.Many location-based services collect current or ongoing location information from a user in order to provide some contextual service (nearest coffee shop; local weather; etc.). Collecting more information than is necessary can harm the user's privacy and increase the risk for the service (in the case of a security breach, for example), but location data may still need to be collected to provide the service. Similarly, users may want the advantages of sharing their location from your service to friends or to some other service, but sharing very precise information provides a much greater risk to users (of re-identification, stalking, physical intrusion,\u00a0etc.).Accepting or transmitting location data at different levels of granularity generally requires a location hierarchy or geographic ontology agreed upon by both services and a more complex data storage model than simple digital\u00a0coordinates.  Truncating latitude and longitude coordinates to a certain number of decimal places may decrease precision, but is generally not considered a good fuzzing algorithm. (For example, if a user is moving in a straight line and regularly updating their location, truncated location information will occasionally reveal precise location when the user crosses a lat/lon boundary.) Similarly, using \"town\" rather than lat/lon may occasionally reveal more precise data than expected when the user crosses a border between two\u00a0towns.Fire Eagle specifically requires that recipient applications be written to handle data at any of the levels, and allows updating the user's location at any level of\u00a0granularity.One of the fore-runners to the W3C Geolocation API, Firefox's experimental Geode feature allowed JavaScript access to the current location at four different levels of\u00a0granularity.", "tags": "", "title": "Location Granularity"},{"url": "/patterns/Strip-invisible-metadata", "text": "Strip potentially sensitive metadata that isn't directly visible to the end\u00a0user.When a service requires a user to import data from external sources (eg.\npictures, tweets, documents) different types of metadata may be \ntransmitted. Users may not be aware of the metadata as it can be\nautomatically generated or not directly visible. Services might be\ninadvertently responsible for exposing private metadata, or going\nagainst users'\u00a0expectations.Users are not always fully aware of the various kinds of metadata\nattached to files and web resources they share with online services.\nMuch of this data is automatically generated, or not directly visible to\nusers during their interactions. This can create situations where, even\nthough users share information explicitly with services, they may be\nsurprised to find this data being revealed. In certain cases where the\ndata is legally protected, the service could be held responsible for any\nleakage of sensitive\u00a0information. How should services that need users to share data and upload files\ntreat additional metadata attached with files? In case of uploading\ndocuments and images, which parts of the metadata can be treated as\nexplicitly shared\u00a0information.Stripping all metadata that is not directly visible during upload time,\nor during the use of the service can help protect services from\nleaks and liabilities. Even in cases where the information is not\nlegally protected, the service can protect themselves from surprising\ntheir users and thus alienating\u00a0them. To summarize: user metadata that can not be made visible to users\nclearly should be stripped to avoid overstepping the users'\u00a0expectations. Twitter.com removes EXIF data from images uploaded to their image\n sharing service. Previously there have been many breaches of personal\n location by using EXIF data shared by image sharing\u00a0services. In certain cases services might build features based on\n metadata, or the metadata sharing could be an important part of the\n community of users. Flickr.com allows users to hide their EXIF data from\n public display, and also provides an interface for users to easily see\n whether they are sharing location as part of uploading their\u00a0images. TODO: add\u00a0screenshots", "tags": "", "title": "Strip Invisible Metadata"},{"url": "/patterns/Aggregation-gateway", "text": "Encrypt, aggregate and decrypt at different\u00a0places.A service provider gets continuous measurements of a service attribute linked to a set of individual service\u00a0users.The provision of a service may require detailed measurements of a service attribute linked to a data subject to adapt the service operation at each moment according to the demand load. However, these measurements may reveal further information (e.g. personal habits, etc.) when repeated over\u00a0time.A feeder metering system can be added as a measuring rod which introduces a comparison for each group of\u00a0meters.Let the service provider have reliable access to the aggregated load at every moment, so as to fulfil its operating requirements, without letting it access the individual load required from each specific service\u00a0user.There is a need to deploy trusted third parties that compute the aggregations over each group of users. Note that they need to be honest (i.e., they cannot collude with the other parties involved), but they need not respect confidentiality (as they only have access to encrypted contents). Smart meters are needed that have computation resources to apply secret generation and homomorphic encryption procedures (note that this is trivial when dealing with the use of computational resources, but it does not have to be always available in the case of e.g. smart grid systems). The potential range of measured values must be large enough to avoid brute force attacks. Robust homomorphic encryption schemes introduce a large computational\u00a0load.An electric utility operates a smart grid network with smart meters that provide measurements of the instantaneous power consumption of each user. The utility employs that information to adapt the power distribution in a dynamic fashion, according to the user demand at each\u00a0moment.", "tags": "", "title": "Aggregation Gateway"},{"url": "/patterns/Privacy-color-coding", "text": "In a social networking site a user gets direct visual cues which\nprivacy settings apply on which shared\u00a0elements.The pattern can be used in applications where users share and publish\npersonal data and contents, but can control their visibility using\nprivacy settings. This includes but is not limited to social\nnetworking\u00a0sites.Privacy settings and the actual effect of these settings on shared\ncontent and data is often not obvious for the user. Not having the\nactive settings constantly in mind might lead to non-optimal privacy\nexperiences when the perceived privacy settings differ from the actual\u00a0settings.The results of privacy settings such as visibility are divided into\ndifferent levels. A distinct color is assigned to each of these\nlevels. Every time the user is performing an action where privacy\nsettings come into play, the color is used as an indication of the\nprivacy settings currently in effect. The choice of colors should take\ninto account prevalent color meanings, like usage of the color red for\nwarning situations. If privacy settings cannot be grouped into\ndistinct levels, a gradient between different colors could also be\u00a0used.Users receive direct visual cues on the consequences of their privacy\nsettings currently in effect. In order to be more clear about their\nprivacy\u00a0settings.Users will directly see the outcome of their privacy settings. The\ndanger of unwanted actions is decreased, as users will permanently\nreceive visual cues. On the other hand a reduction of complex settings\nto a few colors may lead to an oversimplification which would render\nthe whole pattern useless. Visual cues must be integrated into the\nsite design but must still be placed prominently enough to be\nnoticeable. Cultural aspects for the different meanings of colors\nshould be taken into account. The same color may not be recognized as\na warning label in different\u00a0cultures.Alice uses a social network and shares personal stories only with her\nfriends while she shares mundane content publicly. Hence she always\nhas to change the privacy settings of her posts in order to adjust the\nvisibility of the posts. One day she forgets to change the setting and\ndoes not realize that she actually shared a precarious story with her\u00a0boss.", "tags": "", "title": "Privacy Color Coding"},{"url": "/patterns/Active-broadcast-of-presence", "text": "Users may choose actively when they want to share presence information, to increase both the relevance of, and control over,\u00a0sharing.Enable sharing of location or presence data where the user controls the timing and\u00a0context.When a user wants to share or broadcast location or other presence data that is contextually relevant and a full stream of data may be either noisy or\u00a0intrusive.How can a system broadcast a user's presence or location information to a social network without revealing sensitive locations or location histories or overwhelming recipients with noisy\u00a0data?Always requiring active decision-making can burden the user with\u00a0choices.", "tags": "", "title": "Active broadcast of presence"},{"url": "/patterns/Pseudonymous-identity", "text": "Hide the identity by using a pseudonym and ensure a pseudonymous\nidentity that can not be linked with a real identity during online\u00a0interactions.This pattern can be used for systems in which users are identified by\npublic\u00a0identities.Many kinds of sensitive informations are released through web\ninteractions, email, data sharing or location-based systems, which can\ncontain the name of a user or header information in packets. Another\nproblem could be to interact anonymously in a forum. However too much\ninteraction in a forum with an anonymous identity can be dangerous in\nthe sense that the relation between original identity and a\npseudonymous identity can be\u00a0exposed.Initiate a random pseudonym, that can not be related to the original,\nso that the identity is hidden. Furthermore a pseudonym depends on\nconcealment, so the pseudonym allocation needs\u00a0protection.Hide the identity of the\u00a0participants.The real identity of a user is hidden. In certain scenarios there is a\nneed for additional space to store the pseudonym-identity mapping.\nExtensive Usage of the same pseudonym can weaken\u00a0it.Assuming some students are writing an exam and they have to fill out a\nform about their identity, where there is an optional field for a\nchosen pseudonym. This way the result can be released under the chosen\npseudonyms and the identity of each student is hidden. But by being\nobservant, some students might be able to figure out which identity\nbelongs to which pseudonym and so the confidentiality of the identity\nis\u00a0compromised.Anonymizer are well-known tools for anonymous web interactions. They\nwork for example by using a proxy between a request sender and a\nrecipient to strip header information like HTTP_USER_AGENT in packet\nheaders because they contain metadata about packet senders. The\nMixmaster is an anonymous remailer that hides the sender and recipient\nidentity by stripping its name and assigning a pseudonym. Some data\nsharing systems with a privacy-preserving focus make use of pseudonyms\nso that identifying information such as names and social security\nnumbers are hidden. For example various electronic healthcare systems\nare using pseudonyms for the storage of e-health\u00a0records.", "tags": "", "title": "Psuedonymous Identity"},{"url": "/patterns/Policy-matching-display", "text": "Give one careful thought to your privacy needs, then be always able to swiftly apply what you\u00a0decided.A user wants to start using a new service, which lets the user configure several privacy-related parameters. The user often does the same with new, different service\u00a0providers.Users may get overwhelmed by the burden of deciding on privacy aspects each and every time they enrol in a new service. This may make them err on their decisions regarding\u00a0privacy.Before contracting a service, the service provider retrieves the user preferences (exposed by their user agent, or at a well-known URI), and presents the user a comparison between their preferences and the privacy policies applied by default by the service operator, which in turn automatically adapts any configurable values to the user\u2019s declared\u00a0preferences.Allow users to provide a consistent privacy-related behaviour, while reducing their cognitive workload every time they enrol in a new\u00a0service.This pattern requires sharing a machine-readable format to express and exchange definitions of privacy policies between the user agent and the service providers. Several such formats exist, yet they are not always supported by either user agents or by service providers. Besides, not all the privacy policy nuances can be expressed in existing privacy policy\u00a0languages.", "tags": "", "title": "Policy Matching Display"},{"url": "/patterns/Anonymous-reputation-based-blacklisting", "text": "Get rid of troublemakers without even knowing who they\u00a0are.A service provider provides a service to users who access anonymously, and who may make bad use of the\u00a0service.Anonymity is a desirable property from the perspective of privacy. However, anonymity may foster misbehaviour, as users lack any fear of\u00a0retribution.A service provider can assign a reputation score to its users, based on their interactions with the service. Those who misbehave earn a bad reputation, and they are eventually added to a black list and banned from using the service anymore. However, these scoring systems traditionally require the user identity to be disclosed and linked to their reputation score, hence they conflict with anonymity. This has made, for instance, Wikipedia administrators to take the decision to ban edition requests coming from the TOR network, , as they cannot properly identify users who\u00a0misbehave.A Trusted Third Party (TTP) might be introduced in between the user and the service provider. The TTP can receive reputation scores from the service provider so as to enforce reputation-based access policies, while keeping the identity hidden from the service provider. However, this would require the user to trust the TTP not to be a whistle-blower\u00a0indeed.How can we make users accountable for their actions while keeping them\u00a0anonymous?First, the service provider provides their users with credentials for anonymous\u00a0authentication.Then, every time an authenticated user holds a session at the service, the service provider assigns and records a reputation value for that session, depending on the user behaviour during the session. Note that these reputation values can only be linked to a specific session, but not to a specific user (as they have authenticated\u00a0anonymously).When the user comes back and starts a new session at the service, the service provider challenges the user to prove in zero-knowledge that he is not linked to any of the offending sessions (those that have a negative reputation associated). Zero-knowledge proofs allow the user to prove this, without revealing their identity to the service provider. Different, alternative proofs have been proposed, e.g. prove that the user is not linked to any of the sessions in a set of session IDs, prove that the last K sessions of the user have good reputation,\u00a0etc.In practice, more complex blacklisting rules can be applied as well. For instance, several reputation scores can be assigned to the same session, each regarding different facets of the user behaviour. Then, the blacklisting thresholds may take the form of a Boolean combination or a lineal combination over individual session and facet reputation\u00a0values.A service provider wants to prevent users who misbehave from accessing the service anymore, without gaining access to their\u00a0identity.Different implementations may only be practical for services with a reduce number of users, require intense computations, limit the scope of the reputation to a constrained time frame, be vulnerable to Sybil attacks, etc. Nonetheless, protocols are being improved to overcome these and other issues. See the cited sources below for the specific\u00a0discussion.A wiki allows any visitor to modify its contents, even without having been authenticated. Some malicious visitors may vandalize the contents. This fact is signalled by the wiki administrators. If a visitor coming from the same IP address keeps vandalizing the site, they will earn a bad reputation, and their IP will be banned from modifying the contents anymore. However, users accessing through a Tor anonymity network proxy cannot be identified from their IPs, and thus their reputation cannot be\u00a0tracked.", "tags": "", "title": "Anonymous Reputation-based Blacklisting"},{"url": "/patterns/Asynchronous-notice", "text": "How can a service effectively provide notice to a user who gave permission once but whose information is accessed repeatedly (perhaps even continuously) over a long period of time? Proactively notify the user after the time of consent that information is being tracked, stored or\u00a0re-distributed.Support notice of ongoing location\u00a0tracking.How can a service effectively provide notice to a user who gave permission\nonce but whose information is accessed repeatedly (perhaps even continuously)\nover a long period of time? If a user forgets that they gave access (or who\nhas access) they may later be surprised or upset by the continued flow of\npersonal information. Also, initial consent may have been forged by an\nattacker or have been provided by another user of a shared device -- if\nsynchronous notice is only provided at the time of consent, a user may\ninadvertently distribute personal information over a long period of time after\nhaving lost control of their device only\u00a0momentarily.Providing an asynchronous notice requires a reliable mechanism to contact the\nuser (a verified email address or telephone number, for example). Care should\nbe taken to ensure that the mechanism can actually reach the person using the\ndevice being tracked. (For example, notifying the owner of the billing credit\ncard may not help the spouse whose location is being surreptitiously\u00a0tracked.)Many repeated notices may annoy users and eventually inure them to the\npractice altogether. Take measures to avoid unnecessary notices and some level\nof configuration for frequency of notices. This must be balanced against the\nconcerns of an attacker's opting the user in without their\u00a0knowledge.By ensuring that users aren't surprised, asynchronous notice may increase\ntrust in the service and comfort with continued disclosure of\u00a0information.Google Latitude users can configure a reminder email (see below) when their\n location is being shared with any application, including internal applications\n like the Location History\u00a0service.", "tags": "", "title": "Asynchronous notice"},{"url": "/patterns/Privacy-dashboard", "text": "An informational privacy dashboard can provide collected summaries of the collected or processed personal data for a particular\u00a0user.Help users see an overview of the personal information collected about them, particularly when the data or services in question are\u00a0numerous.When your service collects, aggregates or processes personal information from users, particularly information that changes over time, is collected or aggregated in ways that might be unexpected, invisible or easily forgotten, or where users have options for access, correction and\u00a0deletion.How can a service succinctly and effectively communicate the kind and extent of potentially disparate data that has been collected or aggregated by a service? Users may not remember or realize what data a particular service or company has collected, and thus can't be confident that a service isn't collecting too much data. Users who aren't regularly and consistently made aware of what data a service has collected may be surprised or upset when they hear about the service's data collection practices in some other context. Without visibility into the actual data collected, users may not fully understand the abstract description of what types of data are collected; simultaneously, users may easily be overwhelmed by access to raw data without a good understanding of what that data\u00a0means.An informational privacy dashboard can provide collected summaries of the collected or processed personal data for a particular user. While access to raw data may be useful for some purposes, a dashboard provides a summary or highlight of important personal data. Seek to make the data meaningful to the user with examples, visualizations and\u00a0statistics.Where users have choices for deletion or correction of stored data, a dashboard view of collected data is an appropriate place for these controls (which users may be inspired to use on realizing the extent of their collected\u00a0data).In short, a dashboard answers the common user question \"what do you know about me?\" and does so in a way that the user can understand and take appropriate action if\u00a0necessary.Google Accounts: About the\u00a0Dashboard", "tags": "", "title": "Privacy dashboard"},{"url": "/patterns/Anonymity-set", "text": "This pattern aggregates multiple entities into a set, such that they\ncannot be distinguished\u00a0anymore.This pattern is applicable in a messaging scenario, where an attacker\ncan track routing information. Another possible scenario would be the\nstorage of personal information in a\u00a0database.In a system with different users we have the problem that we can often\ndistinguish between them. This enables location tracking, analyzing\nthe behaviour of the users or other privacy-infringing\u00a0practices.There are multiple ways to apply this pattern. One possibility is, to\nstrip away any distinguishing features from the entities. If we do not\nhave enough entities, such that the anonymity set would be too small,\nthen we could even insert fake\u00a0identities.The goal of this pattern is to aggregate different entities into a\nset, such that distinguishing between them becomes\u00a0infeasible.One factor to keep in mind is that this pattern is useless if there\nare not many entities, such that the set of probable suspects is too\nsmall. What \"too small\" means depends on the exact scenario. Another\nfactor is a possible loss of\u00a0functionality.Assuming that there are two companies, one is a treatment clinic for\ncancer and the other one a laboratory for research. The Clinic\nreleases its Protected Health Information (PHI) about cancer victims\nto the laboratory. The PHI's consists of the patients' name, birth\ndate, sex, zip code and diagnostics record. The clinic releases the\ndatasets without the name of the patients, to protect their privacy. A\nmalicious worker at the laboratory for research wants to make use of\nthese informations and recovers the names of the patients. The worker\ngoes to the city council of a certain area to get a voter list from\nthem. The two lists are matched for age, sex and location. The worker\nfinds the name and address information from the voter registration\ndata and the health information from the patient health\u00a0data.Anonymity sets are in use in various routing obfuscation mechanisms\nlike Onion Routing. Hordes is a multicast based protocol, that makes\nuse of multicast routing like point-to-multipoint de- livery, so that\nanonymity is provided. Mix Zone is a location-aware application that\nanonymizes user identity by limiting the positions where users can be\u00a0located.", "tags": "", "title": "Anonymity Set"},{"url": "/patterns/Privacy-icons", "text": "A privacy policy which is hard to understand by general audience is summarized and translated into commonly agreed visual icons. A privacy icon is worth a thousand-word\u00a0policy.This pattern can be applied to any system which collects end user data. It can be presented in an interactive web page but also as part of a physical product which can collect data (e.g. fitness\u00a0tracker)Many organizations provide privacy policies which are too lengthy and hard to understand by the general audience. These policies are oriented as legal disclaimers for legal issues, rather than to inform end users so they can consent to the organization practices after being clearly informed of the collected data, its purpose, and the processing and potential sharing with third\u00a0parties.Include within the service/device a very accessible and visual explanation of the privacy policy. Icons are a great complement to written text, as they may convey much information at a glance through a different modality (images). Standardized icon sets may thus be added to the privacy\u00a0policy.Truly inform customers of the privacy policy of a\u00a0system/organizationUsers may understand, at first glance, what are the potential risks of consenting of a privacy policy. In order to be useful, the icons must be well known and understood by the majority of the potential users before being used. A common meaning of the icon needs to be shared by the community. Educational material can be built upon the implications of each of these\u00a0icons.Alice buys a fitness tracker and she is aware that the device collects her location, and sends it to a central web service in order to provide her with her fitness statistics (her fitness routes, the time spent...). The device provider aggregates this data and provides a business analytics service to third\u00a0parties.Alice is totally unaware of this secondary use of her data and may not agree to it. But accessing this policy involves accessing a website and going through a lengthy and legally oriented\u00a0document.Currently, most of these are only applied by client-side\u00a0solutions.See also the Privacy Icons entry at Ideas for a Better Internet (kind of a pattern repository by the Berkman Center for Internet and Society in\u00a0Harvard).", "tags": "", "title": "Privacy icons"},{"url": "/patterns/Protection-against-tracking", "text": "This pattern avoids the tracking of visitors of websites via cookies.\nIt does this by deleting them at regular intervals or by disabling\ncookies\u00a0completely.This pattern is applicable when personal identifiable information is\ntracked through software tools, protocols or mechanisms such as\ncookies and the\u00a0like.With every single interaction in the web you leave footmarks and clues\nabout yourself. Cookies for example enable webservers to gather\ninformation about web users which therefore affects their privacy and\nanonymity. Web service providers trace user behavior, which can lead\nto user profiling. Also providers can sell the gathered data about\nusers visiting their pages to other\u00a0companies.Restricting usage of cookies on the client side by deleting cookies on\na regular basis e.g. at every start-up of the operating system or\nenabling them case-by-case by deciding if the visited website is\ntrustworthy or not and by accepting a cookie only for the current\nsession. At the highest level of privacy protection cookies are\ndisabled, but as a consequence web services are restricted. Another\nsolution could be that cookies are exchanged between clients, so that\nsophisticated user profiles\u00a0emerge.Restricting a website to not be able to track any of the user's\npersonal identifiable\u00a0informations.With cookies disabled there is no access to sites that require enabled\ncookies for logging in. Other tracking mechanisms for user\nfingerprinting may still work even when cookies are\u00a0disabled.Alice wants to buy shoes and she wants to shop online. She heads to an\nonline shop and searches for shoes but can\u2019t decide which ones she\nwants, so she buys neither of them. The next day she finds a couple of\nemails in her inbox, giving her suggestions for other shoes and\nalerting her that the viewed shoes are now on\u00a0sale.Junkbuster is an old proxy filtering between web server and browser to\nblock ads and cookies, but it is no longer maintained. A program named\nCookieCooker (http://www.cookiecooker.de/) provides protection for\nmonitored user behaviour and interests by exchanging cookies with\nother users or using a random selection of identities. Unfortunately\nthis project also seems to be not maintained anymore. There is also\nthe Firefox Add-on Self-Destructing Cookies which deletes cookies of\ntabs as soon as they are\u00a0closed.", "tags": "", "title": "Protection against Tracking"},{"url": "/patterns/User-data-confinement-pattern", "text": "Avoid the central collection of personal data by shifting some amount\nof the processing of personal data to the user-trusted environments\n(e.g. their own devices). Allow users to control the exact data that\nshares with service\u00a0providersThis pattern may be used whenever the collection of personal data with\none specific and legitimate purpose still pose a relevant level of\nthreat to the users'\u00a0privacyThe engineering process is biased to develop system-centric\narchitectures where the data is collected and processed in single\ncentral entities, forcing users to trust them and share potentially\nsensible personal\u00a0dataThe solution is to shift the trust relationship, meaning that instead\nof having the customer trust the service provide to protect its\npersonal data, the service provider now haves to trust the customers'\u00a0processing.In the smart meter example, the smart meter would receive the monthly\ntariff and calculate the customer's bill which will be then sent to\nthe energy provider where it will be processed. The main benefit is\nthat at no moment the personal data has left the users trusted\u00a0environment.Avoid the need for trust in service providers and the collection of\npersonal\u00a0dataDepending on the type of processing (e.g calculate the bill for the\nmonthly energy consumption or the age from the birth date) the service\nprovider will require some guarantees from the processor (the end\nuser). This may involve the usage of Trusted Platform Modules or\ncryptographic algorithms (e.g.\u00a0ABC4Trust)The smart grid is a domain with a clear example: having smart meters\ndelivering hourly customers' energy consumption to the energy provider\nposes a serious threat to the customers' privacy. If the only purpose\nof collecting these data is to bill the customer, why cannot this\ncalculation be done by the customer based on pre-established\u00a0tariffs?Similar examples in other domains are \"pay as your drive\" insurance\npolicies where the insurance price is calculated based on the drivers\nbehaviour or electronic toll\u00a0pricingSmart meter, Privacy-enhanced attribute based credentials, pay as your\ndrive insurances, electronic toll\u00a0pricing", "tags": "", "title": "User data confinement pattern"},{"url": "/patterns/sticky-policy", "text": "Machine-readable policies are sticked to data to define allowed usage\nand obligations as it travels across multiple parties, enabling users\nto improve control over their personal\u00a0information.Multiple parties are aware of and act according to a certain policy\nwhen privacy-sensitive data is passed along the multiple successive\nparties storing, processing and sharing that\u00a0data.Data may be accessed or handled by multiple parties that share data\nwith an organisation in ways that may not be approved by the data\u00a0subject.Service providers use an obligation management system. Obligation\nmanagement handles information lifecycle management based on\nindividual preferences and organisational policies. The obligation\nmanagement system manipulates data over time, ensuring data\nminimization, deletion and notifications to data\u00a0subjects.The goal of the pattern is to enable users to allow users to control\naccess to their personal\u00a0information.Bene\ufb01ts: Policies can be propagated throughout the cloud to trusted\norganisations, strong enforcement of the policies, traceability.\nLiabilities: Scalability: policies increase size of data. Practicality\nmay not be compatible with existing systems. It may be difficult to\nupdate the policy after sharing of the data and existence of multiple\ncopies of data. It requires ensuring data is handled according to\npolicy e.g. using\u00a0auditing.When data is shared by an organisation they can use privacy preserving\npolicy to enforce respecting user privacy by third party organisations\nthat use, process and store such data. For example, a hospital may\nshare data with third party organisations requiring adhering to\nspecific privacy policies associated with the\u00a0data.", "tags": "", "title": "Sticky Policies"},{"url": "/patterns/Onion-routing", "text": "This pattern provides unlinkability between senders and receivers by\nencapsulating the data in different layers of encryption, limiting the\nknowledge of each node along the delivery\u00a0path.A system in which data is routed between different\u00a0nodes.When delivering data, the receiver has to be known. If the system\nprovides the functionality that the receiver of data should be able to\nanswer, than the receiver should also know the address of the sender.\nWhen forwarding information over multiple stations then, in a naive\nimplementation, each station on the delivery path knows the sender and\nthe final\u00a0destination.The solution is to encrypt the data in layers such that every station\non the way can remove one layer of encryption and thus get to know the\nimmediate next station. This way, every party on the path from the\nsender to the receiver only gets to know the immediate successor and\npredecessor on the delivery\u00a0path.The goal of this pattern is to achieve unlinkability between senders\nand\u00a0receivers.If there are too few hops, the anonymity set is not big enough and the\nunlinkability between sender and receiver is at risk. The same problem\noccurs when there is too few communication going on in the network.\nThe multiple layers of encryption will bloat up the data and consume\nbandwidth. If all nodes on the delivery path collaborate in deducing\nthe sender and the receiver, the pattern becomes\u00a0useless.Alice is a whistle-blower and tries to forward data to Bob who works at\nthe press. She sends the corresponding documents as an\ne-mail-attachment. Eve monitors the traffic and can see who sent this\nmail to whom. The next day, police raids Alice's apartment and sends\nher to jail. Bobs mail account gets\u00a0seized.The TOR-browser, a web-browser specifically designed to ensure\nanonymity makes heavy use of onion\u00a0routing.", "tags": "", "title": "Onion Routing"},{"url": "/patterns/Use-of-dummies", "text": "This pattern hides the actions taken by a user by adding fake actions\nthat are indistinguishable from\u00a0real.This pattern is applicable when it is not possible to avoid executing,\ndelaying or obfuscating the content of an\u00a0action.When users interact with ICT systems their actions reveal a lot of\ninformation about themselves. An option would be for users to not\nperform such actions to protect their privacy. However, this is not\npossible since users cannot completely avoid executing these actions\nbecause they need to perform them to achieve a goal (e.g., search for\na word on the Internet, send an email, search for a\u00a0location).Since the action must be accurately performed, an option to provide\nprivacy is to simultaneously perform other actions in such a way that\nthe adversary cannot distinguish real and fake (often called dummy)\u00a0actions.To hinder the adversary\u2019s ability to infer the user behavior, as well\nas her\u00a0preferences.This pattern entails the need for extra resources to perform the dummy\nactions, both at the side of the user that must repeat the action, and\nat the server side that must process several actions. Sometimes it may\ndegrade the quality of service since the service provider cannot\npersonalize services. It has been demonstrated that generating dummies\nthat are perfectly indistinguishable from real actions (in terms of\ncontent, timing, size, etc...) is very\u00a0difficult.Alice wants to search for an abortion clinic on Google, but she does\nnot want to reveal her intentions of abort to an adversary that may be\neavesdropping this search (e.g., ISP provider, system administrator of\nher workplace,\u00a0etc).The use of this pattern has been proposed to protect privacy in\nlocation based services (the user reveals several locations to the\nservice provider so that her real location is hidden), anonymous\ncommunications (the user sends fake messages to fake recipients to\nhide her profile), web searches (the user searches for fake terms to\nhide her real\u00a0preferences).", "tags": "", "title": "use of dummies"},{"url": "/patterns/identity-federation-do-not-track-pattern", "text": "All information has been extracted from\u00a0http://blog.beejones.net/the-identity-federation-do-not-track-patternThe Do Not Track Pattern makes sure that neither the Identity Provider\nnor the Identity Broker can learn the relationship between the user\nand the Service Providers the user\u00a0us.This pattern is focused on identity federation\u00a0modelsWhen an identity system provides identifying information about a user\nand passes this to a third party service, different parties can do\ncorrelation and derive additional\u00a0information.Include an orchestrator component, that must act in behalf and be\ncontrolled by the user. The orchestrator makes sure that the identity\nbroker can\u2019t correlate the original request from the service provider\nwith the assertions that are returned from the identity provider. The\ncorrelation can only be done within the orchestrator but that\u2019s no\nissue because this acts on behalf of the user, possibly on the device\nof the\u00a0user.Avoid the correlation of end user and service provider\u00a0dataIn practice, the orchestrator could run in the browser of the user as\na javascript program or as an App on his\u00a0deviceIdentity federations and\u00a0ecosystems", "tags": "", "title": "Identity Federation Do Not Track Pattern"},{"url": "/patterns/Encryption-user-managed-keys", "text": "Use encryption in such a way that the service provider cannot decrypt the user's information because the user manages the\u00a0keys.Enable encryption, with user-managed encryption keys, to protect the confidentiality of personal information that may be transferred or stored by an untrusted 3rd\u00a0party.User wants to store or transfer their personal data through an online service and they want to protect their privacy, and specifically the confidentiality of their personal information. Risks of unauthorized access may include the online service provider itself, or third parties such as its partners for example for backup, or government surveillance depending on the geographies the data is stored in or transferred\u00a0through. How can a user store or transfer their personal information through an online service while ensuring their privacy and specifically preventing unauthorized access to their personal\u00a0information?Requiring the user to do encryption key management may annoy or confuse them and they may revert to either no encryption, or encryption with the online service provider managing the encryption key (affording no protection from the specific online service provider managing the key), picking an encryption key that is weak, reused, written down and so\u00a0forth. Some metadata may need to remain unencrypted to support the online service provider or 3rd party functions, for example file names for cloud storage, or routing information for transfer applications, exposing the metadata to risks of unauthorized access, server side indexing for searching, or\u00a0de-duplication. If the service provider has written the client side software that does the client side encryption with a user-managed encryption key, there can be additional concerns regarding whether the client software is secure or tampered with in ways that can compromise\u00a0privacy.Encryption of the personal information of the user prior to storing it with, or transferring it through an online service. In this solution the user shall generate a strong encryption key and manage it themselves, specifically keeping it private and unknown to the untrusted online service or 3rd\u00a0parties.", "tags": "", "title": "Encryption with user-managed keys"},{"url": "/patterns/Trustworthy-privacy-plugin", "text": "Aggregate usage records at the user side in a trustworthy\u00a0manner.A service provider gets continuous measurements of a service attribute linked to a service individual. Applicable service tariffs may vary over\u00a0time.The provision of a service may require repeated, detailed measurements of a service attribute linked to a data subject to e.g. properly bill them for the service usage. However, these measurements may reveal further information (e.g. personal habits, etc.) when repeated over\u00a0time.Host a Privacy Plugin at a consumer-trusted device, in between the metering and the billing systems. and the service provider in charge of billing for the service usage. This privacy plugin, under the consumer\u2019s control, computes the aggregated invoice and sends it to the service provider (or to its billing subsystem), which does not need any fine-grained consumption records anymore. Cryptographic techniques (homomorphic commitments, zero-knowledge proofs of knowledge, digital signatures) are used to ensure trustworthiness of the generated invoices without requiring tamper-proof\u00a0hardware.A service provider can get a trustworthy measurement of service usage along a period to issue a bill for the service usage; however, the detailed consumption for finer intervals cannot be\u00a0obtained.The service provider does not need anymore to access detailed consumption data in order to issue reliable\u00a0bills.An electric utility operates a smart grid network with smart meters that provide measurements of the instantaneous power consumption of each user. Depending on the power demand, dynamic tariffs are applied. The utility employs that information to bill each client periodically, according to his aggregated consumption over the billing period and the respective tariffs at each moment. However, this information can also be exploited to infer sensitive user information (e.g. at what time he or she leaves and comes back to home,\u00a0etc.)", "tags": "", "title": "Trustworthy Privacy Plug-in"},{"url": "/patterns/Layered-policy-design", "text": "Split privacy policies into nested, successively refined versions. Leave the legalese to the\u00a0lawyers.Split privacy policies into nested, successively refined\n    versions. Leave the legalese to the\u00a0lawyers.A data controller offers detailed, legal explanations of their privacy and data protection\u00a0policies.Privacy policies may be difficult to understand and hard to read. What was initially conceived as an instrument to inform users is now almost useless, as they have become riddled with legalese and all sort of extraneous details. As a consequence, users do not read the privacy policies, for being long and\u00a0cumbersome.However, privacy policies are legally binding documents, which makes it difficult to get just rid of these legal\u00a0aspects.A short notice may provide a summary of the practices that deal with personal data, highlighting those which may not be evident to the data subject. Then, a longer policy may provide specific information, split into sections, detailing any uses of personal data. And finally, the whole legal text of the privacy policy can be\u00a0specified.Make users really understand what they can expect about their personal data from a data controller (in terms of which data is managed, for which purposes,\u00a0etc.)The use of this pattern fosters simplicity, transparency and\u00a0choice.However, two versions of the privacy policies coexist, which may introduce potential contradictions; in particular, the data controller must ensure that updates are performed in parallel and\u00a0coherently.See examples at Terms of Service Didn't Read. The average user would take 76 work days to read the privacy policies they encounter each\u00a0year", "tags": "", "title": "Layered Policy Design"}]}